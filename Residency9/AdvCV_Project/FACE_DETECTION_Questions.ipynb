{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgJNvpw9SnZq"
   },
   "source": [
    "#### In this problem we use \"Transfer Learning\" of an Object Detector model to detect any object according to the problem in hand.\n",
    "\n",
    "Here, We are particularly interested in detecting faces in a given image.\n",
    "\n",
    "#### To use the model first, we need to import the model and its supporting files for the model to function. \n",
    "\n",
    "We see the below steps to import the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtnH8CujQfxM"
   },
   "source": [
    "### Import MobileNet model given in file `mn_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtnDz1qkQESM"
   },
   "outputs": [],
   "source": [
    "### Import MobileNet model given in file `mn_model.py`\n",
    "\n",
    "from mn_model import mn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwNy4u8zQk1H"
   },
   "source": [
    "### Import the BatchGenerator and SSDLoss functions in given files `face_generator.py`, `keras_ssd_loss` and `ssd_box_encode_decode_utils.py` as well, used in MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gVMOicwQlg_"
   },
   "outputs": [],
   "source": [
    "#### Import the BatchGenerator and SSDLoss functions as well, used in MobileNet model\n",
    "\n",
    "from face_generator import BatchGenerator\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkzbMq9UeuCM"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K \n",
    "from keras.models import load_model\n",
    "from math import ceil \n",
    "import numpy as np \n",
    "from termcolor import colored\n",
    "\n",
    "from mn_model import mn_model\n",
    "from face_generator import BatchGenerator\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "\n",
    "# training parameters\n",
    "from keras import backend as K\n",
    "import scipy.misc as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqyWOSGJRB18"
   },
   "source": [
    "## Set the parameters for the model\n",
    "\n",
    "#### We need to customize the model parameters according to our problem as given below.\n",
    "\n",
    "#### Set n_classes (no.of classes) = 2, as we are interested in only face detection. \n",
    "#### `Face` will be one class and everything else comes under other class (we can call it as `background`)\n",
    "\n",
    "#### Set class_names = [\"background\", \"face\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcoBM5wlfHgZ"
   },
   "outputs": [],
   "source": [
    "img_height =512\n",
    "img_width = 512\n",
    "\n",
    "img_channels = 3\n",
    "\n",
    "n_classes =2 \n",
    "class_names = [\"background\",\"face\"]\n",
    "\n",
    "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
    "aspect_ratios = [[0.5, 1.0, 2.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [0.5, 1.0, 2.0],\n",
    "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
    "two_boxes_for_ar1 = True\n",
    "limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
    "normalize_coords = True\n",
    "\n",
    "det_model_path = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7vX1GJjTg0v"
   },
   "source": [
    "### Now, we have imported the model and its dependencies. The next thing is to import the dataset for the model to train on. For this, we are using the WIDER FACE dataset. \n",
    "\n",
    "#### To make the dataset available follow the steps given below.\n",
    "\n",
    "\n",
    "1. Create a folder in your google drive for this project. \n",
    "\n",
    "2. Download the train and test dataset files given in .zip format into your drive folder you created for the project in step-1.\n",
    "\n",
    "3. Set the project path variable according to the folders you created to use for this project in your google drive. \n",
    "\n",
    "      `project_path = \"/content/drive/My Drive/DLCP/\"`\n",
    "\n",
    "4. Now, as we mount the drive the images will be available to use for training and testing but in zip format.\n",
    "\n",
    "5. So, lets extract the images from the zipfiles by using the code given of zipfile module.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2du37JKdpAK"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KM1QVQ2pduPE"
   },
   "outputs": [],
   "source": [
    "project_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-pOMEK8pCPS"
   },
   "outputs": [],
   "source": [
    "train_images_path = project_path + 'WIDER_train.zip'\n",
    "test_images_path = project_path + 'WIDER_val.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uIDTr5ApCR7"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "archive = zipfile.ZipFile(train_images_path, 'r')\n",
    "archive.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhqDPEEfpCUr"
   },
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(test_images_path, 'r')\n",
    "archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iCibT8AYiJ7"
   },
   "source": [
    "### Now, the images are available. The next thing we need is to get the labels for these images, so that we can use this information while training for detecting faces with the given model using transfer learning. \n",
    "\n",
    "#### Follow the below steps to get those labels available.\n",
    "\n",
    "\n",
    "Load the  '' `wider_train_small.npy`'' file given to check the information given about the dataset. In this file you can see the information about each image in the dataset in a list with following elemets:\n",
    "      \n",
    "\n",
    "        1.   Image filename (str)\n",
    "        2.   Image filename (str)\n",
    "        3.   Image size (list) [height, width]\n",
    "        4.   List of bounding box co-ordinates and Class label (list) [[a,b,c,d], Class label, ...]\n",
    "        \n",
    "        where,\n",
    "        a,b,c,d are the four co-ordinates of the bounding box\n",
    "        Class label is the position of object as mentioned in `class_names` list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nub3I0LPmlfk"
   },
   "outputs": [],
   "source": [
    "data = np.load('./wider_train_small.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Q2Fd8RY3pCJS",
    "outputId": "b9bbd88c-8772-4753-e12c-3b724adcada8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg\n",
      "['WIDER_train/images/52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg', 'WIDER_train/images/52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg', [300, 300], [[21, 667, 9, 655], 1]]\n"
     ]
    }
   ],
   "source": [
    "### Printed first element to check the above given information.\n",
    "\n",
    "for key in data:\n",
    "    print (key)\n",
    "    print (data[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlTpXziHZwws"
   },
   "source": [
    " As we can see from the above output all the information mentioned above is there for all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2NBAZGqaec9"
   },
   "source": [
    "### Now, load the files `wider_trian.npy` and `wider_val.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kfX49vXas4f"
   },
   "outputs": [],
   "source": [
    "train_data = 'wider_train_small.npy'\n",
    "test_data = 'wider_val_small.npy'\n",
    "\n",
    "x = np.load(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2KdmbvGblC3"
   },
   "source": [
    "### Now, call the imported model with the given parameters and freeze all the layers in the model with names not having ''`detection`'' word as prefix.\n",
    "\n",
    "As we are not training the model from scratch, we are freezing all the above layers in the model having only last few layers while training to update their weights according to the problem in hand. This is called as **Transfer Learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHwplT2ggR-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Model Specific data\n",
      "====> Height, Width, Channels : 512 512 3\n"
     ]
    }
   ],
   "source": [
    "# build the keras model\n",
    "# this model is not retrained, we are doing it from scratch \n",
    "\n",
    "K.clear_session()\n",
    "model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n",
    "                                                                      n_classes = n_classes,\n",
    "                                                                      min_scale = None, \n",
    "                                                                      max_scale = None, \n",
    "                                                                      scales = scales, \n",
    "                                                                      aspect_ratios_global = None, \n",
    "                                                                      aspect_ratios_per_layer = aspect_ratios, \n",
    "                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n",
    "                                                                      limit_boxes=limit_boxes, \n",
    "                                                                      variances= variances, \n",
    "                                                                      coords=coords, \n",
    "                                                                      normalize_coords=normalize_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfC4yNb_uCdT"
   },
   "source": [
    "#### Write code to freeze all the layers in the above model with names not having ''`detection`'' word as prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_1': <keras.engine.input_layer.InputLayer at 0x24a8754b160>,\n",
       " 'lambda1': <keras.layers.core.Lambda at 0x24a8754bd30>,\n",
       " 'lambda2': <keras.layers.core.Lambda at 0x24a8754be48>,\n",
       " 'lambda3': <keras.layers.core.Lambda at 0x24a8754b9b0>,\n",
       " 'conv1': <keras.layers.convolutional.Conv2D at 0x24a8754b710>,\n",
       " 'conv1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8754b898>,\n",
       " 'conv1_relu': <keras.layers.core.Activation at 0x24a8754bc88>,\n",
       " 'conv_dw_1': <mn_model.DepthwiseConv2D at 0x24a8754bfd0>,\n",
       " 'conv_dw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a874f99b0>,\n",
       " 'conv_dw_1_relu': <keras.layers.core.Activation at 0x24afb39c898>,\n",
       " 'conv_pw_1': <keras.layers.convolutional.Conv2D at 0x24a96e2dc88>,\n",
       " 'conv_pw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a98262e80>,\n",
       " 'conv_pw_1_relu': <keras.layers.core.Activation at 0x24a9827e7b8>,\n",
       " 'conv_dw_2': <mn_model.DepthwiseConv2D at 0x24a9830ddd8>,\n",
       " 'conv_dw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a98372588>,\n",
       " 'conv_dw_2_relu': <keras.layers.core.Activation at 0x24a98360f28>,\n",
       " 'conv_pw_2': <keras.layers.convolutional.Conv2D at 0x24a983e2a90>,\n",
       " 'conv_pw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a9842fa58>,\n",
       " 'conv_pw_2_relu': <keras.layers.core.Activation at 0x24a98473ba8>,\n",
       " 'conv_dw_3': <mn_model.DepthwiseConv2D at 0x24a984d6978>,\n",
       " 'conv_dw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a98509eb8>,\n",
       " 'conv_dw_3_relu': <keras.layers.core.Activation at 0x24a98527c50>,\n",
       " 'conv_pw_3': <keras.layers.convolutional.Conv2D at 0x24a985b5ef0>,\n",
       " 'conv_pw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a985eada0>,\n",
       " 'conv_pw_3_relu': <keras.layers.core.Activation at 0x24a98608828>,\n",
       " 'conv_dw_4': <mn_model.DepthwiseConv2D at 0x24a98697d30>,\n",
       " 'conv_dw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a986977f0>,\n",
       " 'conv_dw_4_relu': <keras.layers.core.Activation at 0x24a986e9358>,\n",
       " 'conv_pw_4': <keras.layers.convolutional.Conv2D at 0x24a98758b70>,\n",
       " 'conv_pw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a987bc5f8>,\n",
       " 'conv_pw_4_relu': <keras.layers.core.Activation at 0x24a9883bb38>,\n",
       " 'conv_dw_5': <mn_model.DepthwiseConv2D at 0x24a98863a58>,\n",
       " 'conv_dw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a9889aef0>,\n",
       " 'conv_dw_5_relu': <keras.layers.core.Activation at 0x24a988b7208>,\n",
       " 'conv_pw_5': <keras.layers.convolutional.Conv2D at 0x24a98946c18>,\n",
       " 'conv_pw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a989d4a58>,\n",
       " 'conv_pw_5_relu': <keras.layers.core.Activation at 0x24a989972e8>,\n",
       " 'conv_dw_6': <mn_model.DepthwiseConv2D at 0x24a98a21518>,\n",
       " 'conv_dw_6_bn': <keras.layers.normalization.BatchNormalization at 0x24a98a21828>,\n",
       " 'conv_dw_6_relu': <keras.layers.core.Activation at 0x24a98a71588>,\n",
       " 'conv_pw_6': <keras.layers.convolutional.Conv2D at 0x24a98aeac50>,\n",
       " 'conv_pw_6_bn': <keras.layers.normalization.BatchNormalization at 0x24a98bd8f28>,\n",
       " 'conv_pw_6_relu': <keras.layers.core.Activation at 0x24a98c54ba8>,\n",
       " 'conv_dw_7': <mn_model.DepthwiseConv2D at 0x24a98bfa908>,\n",
       " 'conv_dw_7_bn': <keras.layers.normalization.BatchNormalization at 0x24a98cb6a90>,\n",
       " 'conv_dw_7_relu': <keras.layers.core.Activation at 0x24a98cd1048>,\n",
       " 'conv_pw_7': <keras.layers.convolutional.Conv2D at 0x24a98d96d68>,\n",
       " 'conv_pw_7_bn': <keras.layers.normalization.BatchNormalization at 0x24a98dccb38>,\n",
       " 'conv_pw_7_relu': <keras.layers.core.Activation at 0x24a98db5d30>,\n",
       " 'conv_dw_8': <mn_model.DepthwiseConv2D at 0x24a99e5fdd8>,\n",
       " 'conv_dw_8_bn': <keras.layers.normalization.BatchNormalization at 0x24a99e5f5f8>,\n",
       " 'conv_dw_8_relu': <keras.layers.core.Activation at 0x24a99eaf7f0>,\n",
       " 'conv_pw_8': <keras.layers.convolutional.Conv2D at 0x24a99f2dd30>,\n",
       " 'conv_pw_8_bn': <keras.layers.normalization.BatchNormalization at 0x24a99f88b70>,\n",
       " 'conv_pw_8_relu': <keras.layers.core.Activation at 0x24a9a006cc0>,\n",
       " 'conv_dw_9': <mn_model.DepthwiseConv2D at 0x24a99fa82e8>,\n",
       " 'conv_dw_9_bn': <keras.layers.normalization.BatchNormalization at 0x24a9a069a20>,\n",
       " 'conv_dw_9_relu': <keras.layers.core.Activation at 0x24a9a088f28>,\n",
       " 'conv_pw_9': <keras.layers.convolutional.Conv2D at 0x24a9b13af98>,\n",
       " 'conv_pw_9_bn': <keras.layers.normalization.BatchNormalization at 0x24a9b16f588>,\n",
       " 'conv_pw_9_relu': <keras.layers.core.Activation at 0x24a9b157f98>,\n",
       " 'conv_dw_10': <mn_model.DepthwiseConv2D at 0x24a9b1e02b0>,\n",
       " 'conv_dw_10_bn': <keras.layers.normalization.BatchNormalization at 0x24a98e15f98>,\n",
       " 'conv_dw_10_relu': <keras.layers.core.Activation at 0x24a98e003c8>,\n",
       " 'conv_pw_10': <keras.layers.convolutional.Conv2D at 0x24a8e66af28>,\n",
       " 'conv_pw_10_bn': <keras.layers.normalization.BatchNormalization at 0x24a98b97438>,\n",
       " 'conv_pw_10_relu': <keras.layers.core.Activation at 0x24a98b8ce10>,\n",
       " 'conv_dw_11': <mn_model.DepthwiseConv2D at 0x24a98b670f0>,\n",
       " 'conv_dw_11_bn': <keras.layers.normalization.BatchNormalization at 0x24a98b67358>,\n",
       " 'conv_dw_11_relu': <keras.layers.core.Activation at 0x24a98b55f28>,\n",
       " 'conv_pw_11': <keras.layers.convolutional.Conv2D at 0x24a96e96240>,\n",
       " 'conv_pw_11_bn': <keras.layers.normalization.BatchNormalization at 0x24a96e8b080>,\n",
       " 'conv_pw_11_relu': <keras.layers.core.Activation at 0x24a96bdc3c8>,\n",
       " 'conv_dw_12': <mn_model.DepthwiseConv2D at 0x24a96bcb4a8>,\n",
       " 'conv_dw_12_bn': <keras.layers.normalization.BatchNormalization at 0x24a96bd39b0>,\n",
       " 'conv_dw_12_relu': <keras.layers.core.Activation at 0x24a96bc34e0>,\n",
       " 'conv_pw_12': <keras.layers.convolutional.Conv2D at 0x24a96a10978>,\n",
       " 'conv_pw_12_bn': <keras.layers.normalization.BatchNormalization at 0x24a969e0048>,\n",
       " 'conv_pw_12_relu': <keras.layers.core.Activation at 0x24a96a07cf8>,\n",
       " 'conv_dw_13': <mn_model.DepthwiseConv2D at 0x24a969ea080>,\n",
       " 'conv_dw_13_bn': <keras.layers.normalization.BatchNormalization at 0x24a969d6320>,\n",
       " 'conv_dw_13_relu': <keras.layers.core.Activation at 0x24a969c24a8>,\n",
       " 'conv_pw_13': <keras.layers.convolutional.Conv2D at 0x24a9698c0f0>,\n",
       " 'conv_pw_13_bn': <keras.layers.normalization.BatchNormalization at 0x24a96977390>,\n",
       " 'conv_pw_13_relu': <keras.layers.core.Activation at 0x24a96959630>,\n",
       " 'detection_conv6_1': <keras.layers.convolutional.Conv2D at 0x24a96952ba8>,\n",
       " 'detection_conv6_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a96947f98>,\n",
       " 'detection_conv6_1_nonlin': <keras.layers.core.Lambda at 0x24a9693fc50>,\n",
       " 'detection_conv6_2_conv_dw_1': <mn_model.DepthwiseConv2D at 0x24a9690e8d0>,\n",
       " 'detection_conv6_2_conv_dw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a968fb978>,\n",
       " 'detection_conv6_2_conv_dw_1_relu': <keras.layers.core.Activation at 0x24a8dd136a0>,\n",
       " 'detection_conv6_2_conv_pw_1': <keras.layers.convolutional.Conv2D at 0x24a8dd1d3c8>,\n",
       " 'detection_conv6_2_conv_pw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8dd316a0>,\n",
       " 'detection_conv6_2_conv_pw_1_relu': <keras.layers.core.Activation at 0x24a8dd65cf8>,\n",
       " 'detection_conv7_1': <keras.layers.convolutional.Conv2D at 0x24a8dd6f898>,\n",
       " 'detection_conv7_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8dd78eb8>,\n",
       " 'detection_conv7_1_nonlin': <keras.layers.core.Lambda at 0x24a8dd83470>,\n",
       " 'detection_conv7_2_conv_dw_2': <mn_model.DepthwiseConv2D at 0x24a8ddb6b00>,\n",
       " 'detection_conv7_2_conv_dw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8dd28ac8>,\n",
       " 'detection_conv7_2_conv_dw_2_relu': <keras.layers.core.Activation at 0x24a8ddf1be0>,\n",
       " 'detection_conv7_2_conv_pw_2': <keras.layers.convolutional.Conv2D at 0x24a8de19e10>,\n",
       " 'detection_conv7_2_conv_pw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8de24630>,\n",
       " 'detection_conv7_2_conv_pw_2_relu': <keras.layers.core.Activation at 0x24a8de11128>,\n",
       " 'detection_conv8_1': <keras.layers.convolutional.Conv2D at 0x24a8de3bdd8>,\n",
       " 'detection_conv8_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8de4eba8>,\n",
       " 'detection_conv8_1_nonlin': <keras.layers.core.Lambda at 0x24a8de59780>,\n",
       " 'detection_conv8_2_conv_dw_3': <mn_model.DepthwiseConv2D at 0x24a8dea1518>,\n",
       " 'detection_conv8_2_conv_dw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8decb9e8>,\n",
       " 'detection_conv8_2_conv_dw_3_relu': <keras.layers.core.Activation at 0x24a8dedefd0>,\n",
       " 'detection_conv8_2_conv_pw_3': <keras.layers.convolutional.Conv2D at 0x24a8ded4588>,\n",
       " 'detection_conv8_2_conv_pw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8dee6be0>,\n",
       " 'detection_conv8_2_conv_pw_3_relu': <keras.layers.core.Activation at 0x24a8def37b8>,\n",
       " 'detection_conv9_1': <keras.layers.convolutional.Conv2D at 0x24a8df1de10>,\n",
       " 'detection_conv9_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8df386d8>,\n",
       " 'detection_conv9_1_nonlin': <keras.layers.core.Lambda at 0x24a8df59ef0>,\n",
       " 'detection_conv9_2_conv_dw_4': <mn_model.DepthwiseConv2D at 0x24a8df6e780>,\n",
       " 'detection_conv9_2_conv_dw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8df794e0>,\n",
       " 'detection_conv9_2_conv_dw_4_relu': <keras.layers.core.Activation at 0x24a8dfb4cc0>,\n",
       " 'detection_conv9_2_conv_pw_4': <keras.layers.convolutional.Conv2D at 0x24a8dfbf0f0>,\n",
       " 'detection_conv9_2_conv_pw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8dfcabe0>,\n",
       " 'detection_conv4_3_norm': <keras_layer_L2Normalization.L2Normalization at 0x24a8e026438>,\n",
       " 'detection_conv9_2_conv_pw_4_relu': <keras.layers.core.Activation at 0x24a8dfd49e8>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_dw_1': <mn_model.DepthwiseConv2D at 0x24a8e0394a8>,\n",
       " 'detection_fc7_mbox_conf_conv_dw_2': <mn_model.DepthwiseConv2D at 0x24a8e09f748>,\n",
       " 'detection_conv6_2_mbox_conf_conv_dw_3': <mn_model.DepthwiseConv2D at 0x24a8e0fcb00>,\n",
       " 'detection_conv7_2_mbox_conf_conv_dw_4': <mn_model.DepthwiseConv2D at 0x24a8e1bd0b8>,\n",
       " 'detection_conv8_2_mbox_conf_conv_dw_5': <mn_model.DepthwiseConv2D at 0x24a8e258908>,\n",
       " 'detection_conv9_2_mbox_conf_conv_dw_6': <mn_model.DepthwiseConv2D at 0x24a8e2d4cc0>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_dw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e011d30>,\n",
       " 'detection_fc7_mbox_conf_conv_dw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e0aaa90>,\n",
       " 'detection_conv6_2_mbox_conf_conv_dw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e1389e8>,\n",
       " 'detection_conv7_2_mbox_conf_conv_dw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e1bd7f0>,\n",
       " 'detection_conv8_2_mbox_conf_conv_dw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e2629b0>,\n",
       " 'detection_conv9_2_mbox_conf_conv_dw_6_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e2f1828>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_dw_1': <mn_model.DepthwiseConv2D at 0x24a8e361550>,\n",
       " 'detection_fc7_mbox_loc_conv_dw_2': <mn_model.DepthwiseConv2D at 0x24a8e405ba8>,\n",
       " 'detection_conv6_2_mbox_loc_conv_dw_3': <mn_model.DepthwiseConv2D at 0x24a8e468780>,\n",
       " 'detection_conv7_2_mbox_loc_conv_dw_4': <mn_model.DepthwiseConv2D at 0x24a8e537198>,\n",
       " 'detection_conv8_2_mbox_loc_conv_dw_5': <mn_model.DepthwiseConv2D at 0x24a8e5d2e80>,\n",
       " 'detection_conv9_2_mbox_loc_conv_dw_5': <mn_model.DepthwiseConv2D at 0x24a8e60d048>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_dw_1_relu': <keras.layers.core.Activation at 0x24a8e041ac8>,\n",
       " 'detection_fc7_mbox_conf_conv_dw_2_relu': <keras.layers.core.Activation at 0x24a8e0dd630>,\n",
       " 'detection_conv6_2_mbox_conf_conv_dw_3_relu': <keras.layers.core.Activation at 0x24a8e157630>,\n",
       " 'detection_conv7_2_mbox_conf_conv_dw_4_relu': <keras.layers.core.Activation at 0x24a8e1ef5f8>,\n",
       " 'detection_conv8_2_mbox_conf_conv_dw_5_relu': <keras.layers.core.Activation at 0x24a8e26df98>,\n",
       " 'detection_conv9_2_mbox_conf_conv_dw_6_relu': <keras.layers.core.Activation at 0x24a8e2fcd68>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_dw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e394898>,\n",
       " 'detection_fc7_mbox_loc_conv_dw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e405940>,\n",
       " 'detection_conv6_2_mbox_loc_conv_dw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e4d1d30>,\n",
       " 'detection_conv7_2_mbox_loc_conv_dw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e54ddd8>,\n",
       " 'detection_conv8_2_mbox_loc_conv_dw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e5beb38>,\n",
       " 'detection_conv9_2_mbox_loc_conv_dw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e661390>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_pw_1': <keras.layers.convolutional.Conv2D at 0x24a8e079f60>,\n",
       " 'detection_fc7_mbox_conf_conv_pw_2': <keras.layers.convolutional.Conv2D at 0x24a8e0e66a0>,\n",
       " 'detection_conv6_2_mbox_conf_conv_pw_3': <keras.layers.convolutional.Conv2D at 0x24a8e1786d8>,\n",
       " 'detection_conv7_2_mbox_conf_conv_pw_4': <keras.layers.convolutional.Conv2D at 0x24a8e1d25c0>,\n",
       " 'detection_conv8_2_mbox_conf_conv_pw_5': <keras.layers.convolutional.Conv2D at 0x24a8e294748>,\n",
       " 'detection_conv9_2_mbox_conf_conv_pw_6': <keras.layers.convolutional.Conv2D at 0x24a8e32fbe0>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_dw_1_relu': <keras.layers.core.Activation at 0x24a8e3aa198>,\n",
       " 'detection_fc7_mbox_loc_conv_dw_2_relu': <keras.layers.core.Activation at 0x24a8e439b00>,\n",
       " 'detection_conv6_2_mbox_loc_conv_dw_3_relu': <keras.layers.core.Activation at 0x24a8e4da8d0>,\n",
       " 'detection_conv7_2_mbox_loc_conv_dw_4_relu': <keras.layers.core.Activation at 0x24a8e557f28>,\n",
       " 'detection_conv8_2_mbox_loc_conv_dw_5_relu': <keras.layers.core.Activation at 0x24a8e6242e8>,\n",
       " 'detection_conv9_2_mbox_loc_conv_dw_5_relu': <keras.layers.core.Activation at 0x24a9d095828>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_pw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e094908>,\n",
       " 'detection_fc7_mbox_conf_conv_pw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e0f0400>,\n",
       " 'detection_conv6_2_mbox_conf_conv_pw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e1953c8>,\n",
       " 'detection_conv7_2_mbox_conf_conv_pw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e21b780>,\n",
       " 'detection_conv8_2_mbox_conf_conv_pw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e2aa9b0>,\n",
       " 'detection_conv9_2_mbox_conf_conv_pw_6_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e343c88>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_pw_1': <keras.layers.convolutional.Conv2D at 0x24a8e3c86d8>,\n",
       " 'detection_fc7_mbox_loc_conv_pw_2': <keras.layers.convolutional.Conv2D at 0x24a8e41b6a0>,\n",
       " 'detection_conv6_2_mbox_loc_conv_pw_3': <keras.layers.convolutional.Conv2D at 0x24a8e4bde48>,\n",
       " 'detection_conv7_2_mbox_loc_conv_pw_4': <keras.layers.convolutional.Conv2D at 0x24a8e5412e8>,\n",
       " 'detection_conv8_2_mbox_loc_conv_pw_5': <keras.layers.convolutional.Conv2D at 0x24a8e64ca20>,\n",
       " 'detection_conv9_2_mbox_loc_conv_pw_5': <keras.layers.convolutional.Conv2D at 0x24a9b1f8f60>,\n",
       " 'detection_conv4_3_norm_mbox_conf_conv_pw_1_relu': <keras.layers.core.Activation at 0x24a8e063cc0>,\n",
       " 'detection_fc7_mbox_conf_conv_pw_2_relu': <keras.layers.core.Activation at 0x24a8e126f98>,\n",
       " 'detection_conv6_2_mbox_conf_conv_pw_3_relu': <keras.layers.core.Activation at 0x24a8e18bf60>,\n",
       " 'detection_conv7_2_mbox_conf_conv_pw_4_relu': <keras.layers.core.Activation at 0x24a8e24e3c8>,\n",
       " 'detection_conv8_2_mbox_conf_conv_pw_5_relu': <keras.layers.core.Activation at 0x24a8e2e5da0>,\n",
       " 'detection_conv9_2_mbox_conf_conv_pw_6_relu': <keras.layers.core.Activation at 0x24a8e358c88>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_pw_1_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e3dbeb8>,\n",
       " 'detection_fc7_mbox_loc_conv_pw_2_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e461d30>,\n",
       " 'detection_conv6_2_mbox_loc_conv_pw_3_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e51af28>,\n",
       " 'detection_conv7_2_mbox_loc_conv_pw_4_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e5a99e8>,\n",
       " 'detection_conv8_2_mbox_loc_conv_pw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a8e604c18>,\n",
       " 'detection_conv9_2_mbox_loc_conv_pw_5_bn': <keras.layers.normalization.BatchNormalization at 0x24a9d131a90>,\n",
       " 'detection_conv4_3_norm_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d1ebf98>,\n",
       " 'detection_fc7_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d20ae10>,\n",
       " 'detection_conv6_2_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d20a5c0>,\n",
       " 'detection_conv7_2_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d2c8c88>,\n",
       " 'detection_conv8_2_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d2c8ef0>,\n",
       " 'detection_conv9_2_mbox_conf_reshape': <keras.layers.core.Reshape at 0x24a9d26dfd0>,\n",
       " 'detection_conv4_3_norm_mbox_loc_conv_pw_1_relu': <keras.layers.core.Activation at 0x24a8e3d2e48>,\n",
       " 'detection_fc7_mbox_loc_conv_pw_2_relu': <keras.layers.core.Activation at 0x24a8e495400>,\n",
       " 'detection_conv6_2_mbox_loc_conv_pw_3_relu': <keras.layers.core.Activation at 0x24a8e4f29b0>,\n",
       " 'detection_conv7_2_mbox_loc_conv_pw_4_relu': <keras.layers.core.Activation at 0x24a8e58b828>,\n",
       " 'detection_conv8_2_mbox_loc_conv_pw_5_relu': <keras.layers.core.Activation at 0x24a8e5f1780>,\n",
       " 'detection_conv9_2_mbox_loc_conv_pw_5_relu': <keras.layers.core.Activation at 0x24a9d18aba8>,\n",
       " 'detection_conv4_3_norm_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d1b8940>,\n",
       " 'detection_fc7_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d117be0>,\n",
       " 'detection_conv6_2_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d117f60>,\n",
       " 'detection_conv7_2_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d117a90>,\n",
       " 'detection_conv8_2_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d1ebeb8>,\n",
       " 'detection_conv9_2_mbox_priorbox': <keras_layer_AnchorBoxes.AnchorBoxes at 0x24a9d1ebf28>,\n",
       " 'detection_mbox_conf': <keras.layers.merge.Concatenate at 0x24a9d3ae940>,\n",
       " 'detection_conv4_3_norm_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d295e10>,\n",
       " 'detection_fc7_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d295d30>,\n",
       " 'detection_conv6_2_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d31ec88>,\n",
       " 'detection_conv7_2_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d31e898>,\n",
       " 'detection_conv8_2_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d3425c0>,\n",
       " 'detection_conv9_2_mbox_loc_reshape': <keras.layers.core.Reshape at 0x24a9d342550>,\n",
       " 'detection_conv4_3_norm_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d3652e8>,\n",
       " 'detection_fc7_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d365630>,\n",
       " 'detection_conv6_2_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d365908>,\n",
       " 'detection_conv7_2_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d38d5c0>,\n",
       " 'detection_conv8_2_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d38d208>,\n",
       " 'detection_conv9_2_mbox_priorbox_reshape': <keras.layers.core.Reshape at 0x24a9d3ae080>,\n",
       " 'detection_mbox_conf_softmax': <keras.layers.core.Activation at 0x24a9d3f3780>,\n",
       " 'detection_mbox_loc': <keras.layers.merge.Concatenate at 0x24a9d3d1860>,\n",
       " 'detection_mbox_priorbox': <keras.layers.merge.Concatenate at 0x24a9d3d1898>,\n",
       " 'detection_predictions': <keras.layers.merge.Concatenate at 0x24a9d3f3908>}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snOulB0wt7_t"
   },
   "outputs": [],
   "source": [
    "for layer in model_layer:\n",
    "  if('detection_' not in layer):\n",
    "    #Freezing a layer\n",
    "    model_layer[layer].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "False\n",
      "lambda1\n",
      "False\n",
      "lambda2\n",
      "False\n",
      "lambda3\n",
      "False\n",
      "conv1\n",
      "False\n",
      "conv1_bn\n",
      "False\n",
      "conv1_relu\n",
      "False\n",
      "conv_dw_1\n",
      "False\n",
      "conv_dw_1_bn\n",
      "False\n",
      "conv_dw_1_relu\n",
      "False\n",
      "conv_pw_1\n",
      "False\n",
      "conv_pw_1_bn\n",
      "False\n",
      "conv_pw_1_relu\n",
      "False\n",
      "conv_dw_2\n",
      "False\n",
      "conv_dw_2_bn\n",
      "False\n",
      "conv_dw_2_relu\n",
      "False\n",
      "conv_pw_2\n",
      "False\n",
      "conv_pw_2_bn\n",
      "False\n",
      "conv_pw_2_relu\n",
      "False\n",
      "conv_dw_3\n",
      "False\n",
      "conv_dw_3_bn\n",
      "False\n",
      "conv_dw_3_relu\n",
      "False\n",
      "conv_pw_3\n",
      "False\n",
      "conv_pw_3_bn\n",
      "False\n",
      "conv_pw_3_relu\n",
      "False\n",
      "conv_dw_4\n",
      "False\n",
      "conv_dw_4_bn\n",
      "False\n",
      "conv_dw_4_relu\n",
      "False\n",
      "conv_pw_4\n",
      "False\n",
      "conv_pw_4_bn\n",
      "False\n",
      "conv_pw_4_relu\n",
      "False\n",
      "conv_dw_5\n",
      "False\n",
      "conv_dw_5_bn\n",
      "False\n",
      "conv_dw_5_relu\n",
      "False\n",
      "conv_pw_5\n",
      "False\n",
      "conv_pw_5_bn\n",
      "False\n",
      "conv_pw_5_relu\n",
      "False\n",
      "conv_dw_6\n",
      "False\n",
      "conv_dw_6_bn\n",
      "False\n",
      "conv_dw_6_relu\n",
      "False\n",
      "conv_pw_6\n",
      "False\n",
      "conv_pw_6_bn\n",
      "False\n",
      "conv_pw_6_relu\n",
      "False\n",
      "conv_dw_7\n",
      "False\n",
      "conv_dw_7_bn\n",
      "False\n",
      "conv_dw_7_relu\n",
      "False\n",
      "conv_pw_7\n",
      "False\n",
      "conv_pw_7_bn\n",
      "False\n",
      "conv_pw_7_relu\n",
      "False\n",
      "conv_dw_8\n",
      "False\n",
      "conv_dw_8_bn\n",
      "False\n",
      "conv_dw_8_relu\n",
      "False\n",
      "conv_pw_8\n",
      "False\n",
      "conv_pw_8_bn\n",
      "False\n",
      "conv_pw_8_relu\n",
      "False\n",
      "conv_dw_9\n",
      "False\n",
      "conv_dw_9_bn\n",
      "False\n",
      "conv_dw_9_relu\n",
      "False\n",
      "conv_pw_9\n",
      "False\n",
      "conv_pw_9_bn\n",
      "False\n",
      "conv_pw_9_relu\n",
      "False\n",
      "conv_dw_10\n",
      "False\n",
      "conv_dw_10_bn\n",
      "False\n",
      "conv_dw_10_relu\n",
      "False\n",
      "conv_pw_10\n",
      "False\n",
      "conv_pw_10_bn\n",
      "False\n",
      "conv_pw_10_relu\n",
      "False\n",
      "conv_dw_11\n",
      "False\n",
      "conv_dw_11_bn\n",
      "False\n",
      "conv_dw_11_relu\n",
      "False\n",
      "conv_pw_11\n",
      "False\n",
      "conv_pw_11_bn\n",
      "False\n",
      "conv_pw_11_relu\n",
      "False\n",
      "conv_dw_12\n",
      "False\n",
      "conv_dw_12_bn\n",
      "False\n",
      "conv_dw_12_relu\n",
      "False\n",
      "conv_pw_12\n",
      "False\n",
      "conv_pw_12_bn\n",
      "False\n",
      "conv_pw_12_relu\n",
      "False\n",
      "conv_dw_13\n",
      "False\n",
      "conv_dw_13_bn\n",
      "False\n",
      "conv_dw_13_relu\n",
      "False\n",
      "conv_pw_13\n",
      "False\n",
      "conv_pw_13_bn\n",
      "False\n",
      "conv_pw_13_relu\n",
      "False\n",
      "detection_conv6_1\n",
      "True\n",
      "detection_conv6_1_bn\n",
      "True\n",
      "detection_conv6_1_nonlin\n",
      "True\n",
      "detection_conv6_2_conv_dw_1\n",
      "True\n",
      "detection_conv6_2_conv_dw_1_bn\n",
      "True\n",
      "detection_conv6_2_conv_dw_1_relu\n",
      "True\n",
      "detection_conv6_2_conv_pw_1\n",
      "True\n",
      "detection_conv6_2_conv_pw_1_bn\n",
      "True\n",
      "detection_conv6_2_conv_pw_1_relu\n",
      "True\n",
      "detection_conv7_1\n",
      "True\n",
      "detection_conv7_1_bn\n",
      "True\n",
      "detection_conv7_1_nonlin\n",
      "True\n",
      "detection_conv7_2_conv_dw_2\n",
      "True\n",
      "detection_conv7_2_conv_dw_2_bn\n",
      "True\n",
      "detection_conv7_2_conv_dw_2_relu\n",
      "True\n",
      "detection_conv7_2_conv_pw_2\n",
      "True\n",
      "detection_conv7_2_conv_pw_2_bn\n",
      "True\n",
      "detection_conv7_2_conv_pw_2_relu\n",
      "True\n",
      "detection_conv8_1\n",
      "True\n",
      "detection_conv8_1_bn\n",
      "True\n",
      "detection_conv8_1_nonlin\n",
      "True\n",
      "detection_conv8_2_conv_dw_3\n",
      "True\n",
      "detection_conv8_2_conv_dw_3_bn\n",
      "True\n",
      "detection_conv8_2_conv_dw_3_relu\n",
      "True\n",
      "detection_conv8_2_conv_pw_3\n",
      "True\n",
      "detection_conv8_2_conv_pw_3_bn\n",
      "True\n",
      "detection_conv8_2_conv_pw_3_relu\n",
      "True\n",
      "detection_conv9_1\n",
      "True\n",
      "detection_conv9_1_bn\n",
      "True\n",
      "detection_conv9_1_nonlin\n",
      "True\n",
      "detection_conv9_2_conv_dw_4\n",
      "True\n",
      "detection_conv9_2_conv_dw_4_bn\n",
      "True\n",
      "detection_conv9_2_conv_dw_4_relu\n",
      "True\n",
      "detection_conv9_2_conv_pw_4\n",
      "True\n",
      "detection_conv9_2_conv_pw_4_bn\n",
      "True\n",
      "detection_conv4_3_norm\n",
      "True\n",
      "detection_conv9_2_conv_pw_4_relu\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_dw_1\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_dw_2\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_dw_3\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_dw_4\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_dw_5\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_dw_6\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_dw_1_bn\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_dw_2_bn\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_dw_3_bn\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_dw_4_bn\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_dw_5_bn\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_dw_6_bn\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_dw_1\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_dw_2\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_dw_3\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_dw_4\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_dw_5\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_dw_5\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_dw_1_relu\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_dw_2_relu\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_dw_3_relu\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_dw_4_relu\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_dw_5_relu\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_dw_6_relu\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_dw_1_bn\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_dw_2_bn\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_dw_3_bn\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_dw_4_bn\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_dw_5_bn\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_dw_5_bn\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_pw_1\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_pw_2\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_pw_3\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_pw_4\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_pw_5\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_pw_6\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_dw_1_relu\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_dw_2_relu\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_dw_3_relu\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_dw_4_relu\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_dw_5_relu\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_dw_5_relu\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_pw_1_bn\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_pw_2_bn\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_pw_3_bn\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_pw_4_bn\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_pw_5_bn\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_pw_6_bn\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_pw_1\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_pw_2\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_pw_3\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_pw_4\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_pw_5\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_pw_5\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_conv_pw_1_relu\n",
      "True\n",
      "detection_fc7_mbox_conf_conv_pw_2_relu\n",
      "True\n",
      "detection_conv6_2_mbox_conf_conv_pw_3_relu\n",
      "True\n",
      "detection_conv7_2_mbox_conf_conv_pw_4_relu\n",
      "True\n",
      "detection_conv8_2_mbox_conf_conv_pw_5_relu\n",
      "True\n",
      "detection_conv9_2_mbox_conf_conv_pw_6_relu\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_pw_1_bn\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_pw_2_bn\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_pw_3_bn\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_pw_4_bn\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_pw_5_bn\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_pw_5_bn\n",
      "True\n",
      "detection_conv4_3_norm_mbox_conf_reshape\n",
      "True\n",
      "detection_fc7_mbox_conf_reshape\n",
      "True\n",
      "detection_conv6_2_mbox_conf_reshape\n",
      "True\n",
      "detection_conv7_2_mbox_conf_reshape\n",
      "True\n",
      "detection_conv8_2_mbox_conf_reshape\n",
      "True\n",
      "detection_conv9_2_mbox_conf_reshape\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_conv_pw_1_relu\n",
      "True\n",
      "detection_fc7_mbox_loc_conv_pw_2_relu\n",
      "True\n",
      "detection_conv6_2_mbox_loc_conv_pw_3_relu\n",
      "True\n",
      "detection_conv7_2_mbox_loc_conv_pw_4_relu\n",
      "True\n",
      "detection_conv8_2_mbox_loc_conv_pw_5_relu\n",
      "True\n",
      "detection_conv9_2_mbox_loc_conv_pw_5_relu\n",
      "True\n",
      "detection_conv4_3_norm_mbox_priorbox\n",
      "True\n",
      "detection_fc7_mbox_priorbox\n",
      "True\n",
      "detection_conv6_2_mbox_priorbox\n",
      "True\n",
      "detection_conv7_2_mbox_priorbox\n",
      "True\n",
      "detection_conv8_2_mbox_priorbox\n",
      "True\n",
      "detection_conv9_2_mbox_priorbox\n",
      "True\n",
      "detection_mbox_conf\n",
      "True\n",
      "detection_conv4_3_norm_mbox_loc_reshape\n",
      "True\n",
      "detection_fc7_mbox_loc_reshape\n",
      "True\n",
      "detection_conv6_2_mbox_loc_reshape\n",
      "True\n",
      "detection_conv7_2_mbox_loc_reshape\n",
      "True\n",
      "detection_conv8_2_mbox_loc_reshape\n",
      "True\n",
      "detection_conv9_2_mbox_loc_reshape\n",
      "True\n",
      "detection_conv4_3_norm_mbox_priorbox_reshape\n",
      "True\n",
      "detection_fc7_mbox_priorbox_reshape\n",
      "True\n",
      "detection_conv6_2_mbox_priorbox_reshape\n",
      "True\n",
      "detection_conv7_2_mbox_priorbox_reshape\n",
      "True\n",
      "detection_conv8_2_mbox_priorbox_reshape\n",
      "True\n",
      "detection_conv9_2_mbox_priorbox_reshape\n",
      "True\n",
      "detection_mbox_conf_softmax\n",
      "True\n",
      "detection_mbox_loc\n",
      "True\n",
      "detection_mbox_priorbox\n",
      "True\n",
      "detection_predictions\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Module to print colourful statements\n",
    "from termcolor import colored\n",
    "\n",
    "#Check which layers have been frozen \n",
    "for layer in model.layers:\n",
    "  print (colored(layer.name, color='blue'))\n",
    "  print (colored(layer.trainable, color='red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1IIGWM2c3x-"
   },
   "source": [
    "### After making the model ready for transfer learning, load the weights of the model given in file ''`mobilenet_1_0_224_tf.h5`''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmCmRr2Rc2Sv"
   },
   "outputs": [],
   "source": [
    "model.load_weights('mobilenet_1_0_224_tf.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73KNzDSCf6Rh"
   },
   "source": [
    "#### Using the functions given in the model, we are trying to divide the dataset into train and validation samples. Run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4_pZEU8TfBoR",
    "outputId": "6490ca74-d3f7-463d-f98a-f337d9fb6433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>TRAINING DATA\n",
      "==> Parsing XML files ...\n",
      "==>Parsing XML Finished.\n",
      "==>Generate training batches...\n",
      "==>Training batch generation complete\n",
      "==>Total number of training samples = 128\n",
      "==>VALIDATION\n",
      "==> Parsing XML files ...\n",
      "==>Parsing XML Finished.\n",
      "==>Generate training batches...\n",
      "==>Training batch generation complete\n",
      "==>Total number of validation samples = 60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
    "                                img_width=img_width,\n",
    "                                n_classes=n_classes, \n",
    "                                predictor_sizes=predictor_sizes,\n",
    "                                min_scale=None,\n",
    "                                max_scale=None,\n",
    "                                scales=scales,\n",
    "                                aspect_ratios_global=None,\n",
    "                                aspect_ratios_per_layer=aspect_ratios,\n",
    "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                limit_boxes=limit_boxes,\n",
    "                                variances=variances,\n",
    "                                pos_iou_threshold=0.5,\n",
    "                                neg_iou_threshold=0.2,\n",
    "                                coords=coords,\n",
    "                                normalize_coords=normalize_coords)\n",
    "\n",
    "train_dataset = BatchGenerator(images_path=train_data, \n",
    "                include_classes='all', \n",
    "                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "print (\"==>TRAINING DATA\")\n",
    "print (\"==> Parsing XML files ...\")\n",
    "\n",
    "train_dataset.parse_xml(\n",
    "                  annotations_path=train_data,\n",
    "                  image_set_path='None',\n",
    "                  image_set='None',\n",
    "                  classes = class_names, \n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False, \n",
    "                  debug = False)\n",
    "print(\"==>Parsing XML Finished.\")\n",
    "\n",
    "print (\"==>Generate training batches...\")\n",
    "train_generator = train_dataset.generate(\n",
    "                 batch_size=batch_size,\n",
    "                 train=True,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=True,\n",
    "                 brightness=(0.5,2,0.5),\n",
    "                 flip=0.5,\n",
    "                 translate=((0, 20), (0, 30), 0.5),\n",
    "                 scale=(0.75, 1.2, 0.5),\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False,\n",
    "                 resize=(img_height, img_width),\n",
    "                 #resize=False,\n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "print (\"==>Training batch generation complete\")\n",
    "\n",
    "n_train_samples = train_dataset.get_n_samples()\n",
    "\n",
    "print (\"==>Total number of training samples = {}\".format(n_train_samples))\n",
    "\n",
    "# Now repeat above steps for validation data \n",
    "\n",
    "print (\"==>VALIDATION\")\n",
    "\n",
    "val_dataset = BatchGenerator(images_path=test_data, include_classes='all', \n",
    "                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "print (\"==> Parsing XML files ...\")\n",
    "\n",
    "\n",
    "val_dataset.parse_xml(\n",
    "                  annotations_path=test_data,\n",
    "                  image_set_path='None',\n",
    "                  image_set='None',\n",
    "                  classes = class_names, \n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False, \n",
    "                  debug = False)\n",
    "\n",
    "\n",
    "print(\"==>Parsing XML Finished.\")\n",
    "\n",
    "\n",
    "print (\"==>Generate training batches...\")\n",
    "val_generator = val_dataset.generate(\n",
    "                 batch_size=batch_size,\n",
    "                 train=True,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=False,\n",
    "                 brightness=False,\n",
    "                 flip=False,\n",
    "                 translate=False,\n",
    "                 scale=False,\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False, \n",
    "                 resize=(img_height, img_width), \n",
    "                 #resize=False, \n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "\n",
    "print (\"==>Training batch generation complete\")\n",
    "\n",
    "n_val_samples = val_dataset.get_n_samples()\n",
    "\n",
    "print (\"==>Total number of validation samples = {}\".format(n_val_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KtkdFTQhKlE"
   },
   "source": [
    "### Now, lets setup things for training by initilaizing required variables like learning rate, epochs, optimizer and loss function(SSDLoss) to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1-3OOPmgmxk"
   },
   "outputs": [],
   "source": [
    "# setting up training \n",
    "\n",
    "# batch_size and no.of epochs\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "#Learning rate\n",
    "base_lr = 0.002\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr=base_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay = 0.0)\n",
    "\n",
    "# Loss\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=2, n_neg_min=0, alpha=1.0, beta = 1.0)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vt_RxSHJitPR"
   },
   "source": [
    "### Lets add early stopping and model checkpoint layers on validation loss with some patience values and using fit_generator function to train the model on data generated batch-by-batch by a Python generator, `train_generator` object as generator.\n",
    "\n",
    "\n",
    "We are using checkpoint to save the best model based on validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugJ68M7_vDqy"
   },
   "source": [
    "#### Write code for early_stopping and model_checkpoint layers. Using model.fit_generator train the model and save the best weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeO-T9jwi8be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 22:09 - loss: 0.2972 - acc: 0.06 - ETA: 16:00 - loss: 0.2574 - acc: 0.06 - ETA: 13:32 - loss: 0.2582 - acc: 0.06 - ETA: 13:36 - loss: 0.2643 - acc: 0.06 - ETA: 11:32 - loss: 0.2643 - acc: 0.06 - ETA: 10:09 - loss: 0.2624 - acc: 0.06 - ETA: 9:05 - loss: 0.2596 - acc: 0.0696 - ETA: 7:57 - loss: 0.2590 - acc: 0.070 - ETA: 7:09 - loss: 0.2592 - acc: 0.070 - ETA: 6:00 - loss: 0.2601 - acc: 0.070 - ETA: 5:01 - loss: 0.2587 - acc: 0.070 - ETA: 3:59 - loss: 0.2562 - acc: 0.070 - ETA: 2:59 - loss: 0.2538 - acc: 0.070 - ETA: 2:00 - loss: 0.2538 - acc: 0.071 - ETA: 59s - loss: 0.2542 - acc: 0.071 - 1054s 66s/step - loss: 0.2494 - acc: 0.0713 - val_loss: 0.2764 - val_acc: 0.0550\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 0.27638, saving model to model-0.06.h5\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - ETA: 16:14 - loss: 0.2378 - acc: 0.07 - ETA: 13:47 - loss: 0.2370 - acc: 0.07 - ETA: 11:56 - loss: 0.2345 - acc: 0.07 - ETA: 10:45 - loss: 0.2391 - acc: 0.07 - ETA: 9:43 - loss: 0.2324 - acc: 0.0754 - ETA: 9:28 - loss: 0.2339 - acc: 0.075 - ETA: 8:25 - loss: 0.2324 - acc: 0.075 - ETA: 7:30 - loss: 0.2349 - acc: 0.075 - ETA: 6:30 - loss: 0.2322 - acc: 0.075 - ETA: 5:31 - loss: 0.2283 - acc: 0.076 - ETA: 4:34 - loss: 0.2304 - acc: 0.076 - ETA: 3:40 - loss: 0.2303 - acc: 0.076 - ETA: 2:45 - loss: 0.2309 - acc: 0.077 - ETA: 1:50 - loss: 0.2275 - acc: 0.077 - ETA: 55s - loss: 0.2267 - acc: 0.077 - 1040s 65s/step - loss: 0.2287 - acc: 0.0780 - val_loss: 0.2602 - val_acc: 0.0640\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.27638\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 13:20 - loss: 0.2378 - acc: 0.08 - ETA: 12:24 - loss: 0.2278 - acc: 0.08 - ETA: 11:59 - loss: 0.2280 - acc: 0.08 - ETA: 10:41 - loss: 0.2277 - acc: 0.08 - ETA: 9:40 - loss: 0.2234 - acc: 0.0853 - ETA: 9:06 - loss: 0.2291 - acc: 0.085 - ETA: 8:14 - loss: 0.2244 - acc: 0.085 - ETA: 7:22 - loss: 0.2202 - acc: 0.085 - ETA: 6:42 - loss: 0.2213 - acc: 0.085 - ETA: 5:43 - loss: 0.2189 - acc: 0.085 - ETA: 4:53 - loss: 0.2185 - acc: 0.085 - ETA: 3:59 - loss: 0.2158 - acc: 0.085 - ETA: 2:57 - loss: 0.2184 - acc: 0.086 - ETA: 1:58 - loss: 0.2205 - acc: 0.086 - ETA: 58s - loss: 0.2190 - acc: 0.086 - 1038s 65s/step - loss: 0.2176 - acc: 0.0864 - val_loss: 0.2417 - val_acc: 0.0674\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27638\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 13:50 - loss: 0.2199 - acc: 0.08 - ETA: 13:03 - loss: 0.2225 - acc: 0.08 - ETA: 12:09 - loss: 0.2117 - acc: 0.08 - ETA: 11:07 - loss: 0.2087 - acc: 0.08 - ETA: 10:11 - loss: 0.2115 - acc: 0.08 - ETA: 9:41 - loss: 0.2035 - acc: 0.0878 - ETA: 8:33 - loss: 0.2085 - acc: 0.088 - ETA: 7:55 - loss: 0.2104 - acc: 0.088 - ETA: 6:55 - loss: 0.2105 - acc: 0.088 - ETA: 5:54 - loss: 0.2111 - acc: 0.088 - ETA: 4:51 - loss: 0.2108 - acc: 0.088 - ETA: 3:50 - loss: 0.2095 - acc: 0.088 - ETA: 2:56 - loss: 0.2103 - acc: 0.088 - ETA: 1:56 - loss: 0.2106 - acc: 0.088 - ETA: 58s - loss: 0.2105 - acc: 0.088 - 1052s 66s/step - loss: 0.2080 - acc: 0.0889 - val_loss: 0.2299 - val_acc: 0.0648\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27638\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 19:23 - loss: 0.2185 - acc: 0.09 - ETA: 15:29 - loss: 0.2242 - acc: 0.09 - ETA: 13:16 - loss: 0.2264 - acc: 0.09 - ETA: 11:50 - loss: 0.2174 - acc: 0.09 - ETA: 10:39 - loss: 0.2117 - acc: 0.09 - ETA: 9:39 - loss: 0.2037 - acc: 0.0903 - ETA: 8:35 - loss: 0.2041 - acc: 0.090 - ETA: 7:27 - loss: 0.2076 - acc: 0.090 - ETA: 6:27 - loss: 0.2097 - acc: 0.090 - ETA: 5:26 - loss: 0.2114 - acc: 0.090 - ETA: 4:32 - loss: 0.2115 - acc: 0.090 - ETA: 3:34 - loss: 0.2098 - acc: 0.089 - ETA: 2:38 - loss: 0.2092 - acc: 0.089 - ETA: 1:50 - loss: 0.2106 - acc: 0.089 - ETA: 55s - loss: 0.2089 - acc: 0.089 - 994s 62s/step - loss: 0.2088 - acc: 0.0896 - val_loss: 0.2387 - val_acc: 0.0614\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27638\n",
      "Epoch 6/10\n",
      " 7/16 [============>.................] - ETA: 14:30 - loss: 0.2215 - acc: 0.08 - ETA: 13:01 - loss: 0.2094 - acc: 0.08 - ETA: 11:57 - loss: 0.1993 - acc: 0.08 - ETA: 10:40 - loss: 0.1940 - acc: 0.08 - ETA: 10:23 - loss: 0.2047 - acc: 0.08 - ETA: 9:49 - loss: 0.2073 - acc: 0.0890 - ETA: 8:37 - loss: 0.1995 - acc: 0.0894"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"model-{val_acc:.2f}.h5\", verbose=1, save_best_only=True,\n",
    "                              save_weights_only=True, mode=\"max\", period=1) # Checkpoint best validation model\n",
    "stop = EarlyStopping(monitor=\"val_acc\", patience=100, mode=\"max\") # Stop early, if the validation error deteriorates\n",
    "\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                              steps_per_epoch = ceil(n_train_samples/batch_size)*2,\n",
    "                              epochs = num_epochs,\n",
    "                              callbacks = [checkpoint, stop],                      \n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = ceil(n_val_samples/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3FHrp77jdOx"
   },
   "source": [
    "### Load the best saved model from above step and check predictions for test data using test_generator object to generate batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7eP30cxvcEA"
   },
   "source": [
    "#### Write code in the below cell to load best saved model in the above step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nh7tMrxFjR4B"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path + model_name,  by_name= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dD7q7pzqvnh"
   },
   "source": [
    "### Use the below function to plot the boundingbox in the test image to show the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h89fe5NGqwI9"
   },
   "outputs": [],
   "source": [
    "def save_bb(path, filename, results, prediction=True):\n",
    "  \n",
    "  # print filename\n",
    "\n",
    "  img = image.load_img(filename, target_size=(img_height, img_width))\n",
    "  img = image.img_to_array(img)\n",
    "\n",
    "  filename = filename.split(\"/\")[-1]\n",
    "\n",
    "  if(not prediction):\n",
    "    filename = filename[:-4] + \"_gt\" + \".jpg\"\n",
    "\n",
    "  #fig,currentAxis = plt.subplots(1)\n",
    "  currentAxis = plt.gca()\n",
    "\n",
    " # Get detections with confidence higher than 0.6.\n",
    "  colors = plt.cm.hsv(np.linspace(0, 1, 25)).tolist()\n",
    "  color_code = min(len(results), 16)\n",
    "  print (colored(\"total number of bbs: %d\" % len(results), \"yellow\"))\n",
    "  for result in results:\n",
    "    # Parse the outputs.\n",
    "\n",
    "    if(prediction):\n",
    "      det_label = result[0]\n",
    "      det_conf = result[1]\n",
    "      det_xmin = result[2]\n",
    "      det_xmax = result[3]\n",
    "      det_ymin = result[4]\n",
    "      det_ymax = result[5]\n",
    "    else :\n",
    "      det_label = result[0]\n",
    "      det_xmin = result[1]\n",
    "      det_xmax = result[2]\n",
    "      det_ymin = result[3]\n",
    "      det_ymax = result[4]\n",
    "\n",
    "    xmin = int(det_xmin)\n",
    "    ymin = int(det_ymin)\n",
    "    xmax = int(det_xmax)\n",
    "    ymax = int(det_ymax)\n",
    "\n",
    "    if(prediction):\n",
    "      score = det_conf\n",
    "    \n",
    "    plt.imshow(img / 255.)\n",
    "    \n",
    "    label = int(int(det_label))\n",
    "    label_name = class_names[label]\n",
    "    # print label_name \n",
    "    # print label\n",
    "\n",
    "    if(prediction):\n",
    "      display_txt = '{:0.2f}'.format(score)\n",
    "    else:\n",
    "      display_txt = '{}'.format(label_name)\n",
    "\n",
    "      \n",
    "    # print (xmin, ymin, ymin, ymax)\n",
    "    coords = (xmin, ymin), (xmax-xmin), (ymax-ymin)\n",
    "    color_code = color_code-1 \n",
    "    color = colors[color_code]\n",
    "    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.2})\n",
    "\n",
    "  # y\n",
    "  currentAxis.axes.get_yaxis().set_visible(False)\n",
    "  # x\n",
    "  currentAxis.axes.get_xaxis().set_visible(False)\n",
    "  plt.savefig(path + filename, bbox_inches='tight')\n",
    "\n",
    "  print ('saved' , path + filename)\n",
    "\n",
    "  plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrMdNhMtwE3t"
   },
   "source": [
    "#### Run the below code to create a folder with name output_test and get the predictions for the test images using model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Sep0lG3vR0R"
   },
   "outputs": [],
   "source": [
    "!mkdir output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70s6RPuInpqo"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "test_size = 16\n",
    "test_generator = val_dataset.generate(\n",
    "                 batch_size=test_size,\n",
    "                 train=False,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=False,\n",
    "                 brightness=False,\n",
    "                 flip=False,\n",
    "                 translate=False,\n",
    "                 scale=False,\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False, \n",
    "                 resize=(img_height, img_width), \n",
    "                 #resize=False,\n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "print (colored(\"done.\", \"green\"))\n",
    "\n",
    "print (colored(\"now predicting...\", \"yellow\"))\n",
    "\n",
    "_CONF = 0.60 \n",
    "_IOU = 0.15\n",
    "\n",
    "for i in range(test_size):\n",
    "    X, y, filenames = next(test_generator)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_decoded = decode_y2(y_pred,\n",
    "                             confidence_thresh=_CONF,\n",
    "                            iou_threshold=_IOU,\n",
    "                            top_k='all',\n",
    "                            input_coords=coords,\n",
    "                            normalize_coords=normalize_coords,\n",
    "                            img_height=img_height,\n",
    "                            img_width=img_width)\n",
    "\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n",
    "    save_bb(\"./output_test/\", filenames[i], y[i], prediction=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLKgtPCJwena"
   },
   "source": [
    "In the above step all the test images along with predictions are stored in output_test folder in this notebook environment. You can check the folder in Files section of the menu to left-side of screen in colab.\n",
    "\n",
    "Each test image is used for predictions and is stored as 2 files.\n",
    "\n",
    "one file is the original ground truth with <filename_gt.jpg>\n",
    "second file is the prediction of the model on the image. with <filename.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TFzb3Vj9kDGH"
   },
   "source": [
    "### Visualize a test image to check predictions\n",
    "\n",
    "\n",
    "#### Write code to show images: Using cv2.imshow() or matplotlib show any 3 test images and their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zA1lMnX4-edG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FACE DETECTION_Questions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
