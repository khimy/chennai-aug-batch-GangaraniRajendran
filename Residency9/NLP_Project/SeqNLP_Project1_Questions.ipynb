{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "\n",
    "### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "\n",
    "It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "`from keras.datasets import imdb`\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "\n",
    "### Aim\n",
    "\n",
    "1. Import test and train data  \n",
    "2. Import the labels ( train and test) \n",
    "3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n",
    "4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n",
    "5. Report the Accuracy of the model. (5 points)  \n",
    "6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "#### Usage:\n",
    "1. Import test and train data \n",
    "2. Import the labels ( train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iyyappan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (25000, 300)\n",
      "y_train shape:  (25000,)\n",
      "x_test shape:  (25000, 300)\n",
      "y_test shape:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train shape: \", x_train.shape)\n",
    "print (\"y_train shape: \", y_train.shape)\n",
    "print (\"x_test shape: \", x_test.shape)\n",
    "print (\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of a word index \n",
      "9999\n",
      "Maximum length num words of review in train \n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum value of a word index \")\n",
    "print(max([max(sequence) for sequence in x_train]))\n",
    "print(\"Maximum length num words of review in train \")\n",
    "print(max([len(sequence) for sequence in x_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get the word index and then Create key value pair for word and word_id. (12.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Word to ID dictionary\n",
    "INDEX_FROM=3   # word index offset\n",
    "word_to_id = imdb.get_word_index() #Get the word index\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"[PAD]\"] = 0\n",
    "#word_to_id[\"\"] = 0\n",
    "word_to_id[\"[üèÉ]\"] = 1 # START\n",
    "word_to_id[\"[‚ùì]\"] = 2 # UNKNOWN\n",
    "\n",
    "# Make ID to Word dictionary\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "def restore_original_text(imdb_x_array):\n",
    "    return (' '.join(id_to_word[id] for id in imdb_x_array ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34704: 'fawn',\n",
       " 52009: 'tsukino',\n",
       " 52010: 'nunnery',\n",
       " 16819: 'sonja',\n",
       " 63954: 'vani',\n",
       " 1411: 'woods',\n",
       " 16118: 'spiders',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 52011: 'trawling',\n",
       " 52012: \"hold's\",\n",
       " 11310: 'comically',\n",
       " 40833: 'localized',\n",
       " 30571: 'disobeying',\n",
       " 52013: \"'royale\",\n",
       " 40834: \"harpo's\",\n",
       " 52014: 'canet',\n",
       " 19316: 'aileen',\n",
       " 52015: 'acurately',\n",
       " 52016: \"diplomat's\",\n",
       " 25245: 'rickman',\n",
       " 6749: 'arranged',\n",
       " 52017: 'rumbustious',\n",
       " 52018: 'familiarness',\n",
       " 52019: \"spider'\",\n",
       " 68807: 'hahahah',\n",
       " 52020: \"wood'\",\n",
       " 40836: 'transvestism',\n",
       " 34705: \"hangin'\",\n",
       " 2341: 'bringing',\n",
       " 40837: 'seamier',\n",
       " 34706: 'wooded',\n",
       " 52021: 'bravora',\n",
       " 16820: 'grueling',\n",
       " 1639: 'wooden',\n",
       " 16821: 'wednesday',\n",
       " 52022: \"'prix\",\n",
       " 34707: 'altagracia',\n",
       " 52023: 'circuitry',\n",
       " 11588: 'crotch',\n",
       " 57769: 'busybody',\n",
       " 52024: \"tart'n'tangy\",\n",
       " 14132: 'burgade',\n",
       " 52026: 'thrace',\n",
       " 11041: \"tom's\",\n",
       " 52028: 'snuggles',\n",
       " 29117: 'francesco',\n",
       " 52030: 'complainers',\n",
       " 52128: 'templarios',\n",
       " 40838: '272',\n",
       " 52031: '273',\n",
       " 52133: 'zaniacs',\n",
       " 34709: '275',\n",
       " 27634: 'consenting',\n",
       " 40839: 'snuggled',\n",
       " 15495: 'inanimate',\n",
       " 52033: 'uality',\n",
       " 11929: 'bronte',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 52034: \"yomada's\",\n",
       " 34710: \"madman's\",\n",
       " 30588: 'dialoge',\n",
       " 52036: 'usenet',\n",
       " 40840: 'videodrome',\n",
       " 26341: \"kid'\",\n",
       " 52037: 'pawed',\n",
       " 30572: \"'girlfriend'\",\n",
       " 52038: \"'pleasure\",\n",
       " 52039: \"'reloaded'\",\n",
       " 40842: \"kazakos'\",\n",
       " 52040: 'rocque',\n",
       " 52041: 'mailings',\n",
       " 11930: 'brainwashed',\n",
       " 16822: 'mcanally',\n",
       " 52042: \"tom''\",\n",
       " 25246: 'kurupt',\n",
       " 21908: 'affiliated',\n",
       " 52043: 'babaganoosh',\n",
       " 40843: \"noe's\",\n",
       " 40844: 'quart',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 21909: 'kida',\n",
       " 23382: 'kidd',\n",
       " 52044: \"error'\",\n",
       " 52045: 'neurologist',\n",
       " 18513: 'spotty',\n",
       " 30573: 'cobblers',\n",
       " 9881: 'projection',\n",
       " 40845: 'fastforwarding',\n",
       " 52046: 'sters',\n",
       " 52047: \"eggar's\",\n",
       " 52048: 'etherything',\n",
       " 40846: 'gateshead',\n",
       " 34711: 'airball',\n",
       " 25247: 'unsinkable',\n",
       " 7183: 'stern',\n",
       " 52049: \"cervi's\",\n",
       " 40847: 'dnd',\n",
       " 11589: 'dna',\n",
       " 20601: 'insecurity',\n",
       " 52050: \"'reboot'\",\n",
       " 11040: 'trelkovsky',\n",
       " 52051: 'jaekel',\n",
       " 52052: 'sidebars',\n",
       " 52053: \"sforza's\",\n",
       " 17636: 'distortions',\n",
       " 52054: 'mutinies',\n",
       " 30605: 'sermons',\n",
       " 40849: '7ft',\n",
       " 52055: 'boobage',\n",
       " 52056: \"o'bannon's\",\n",
       " 23383: 'populations',\n",
       " 52057: 'chulak',\n",
       " 27636: 'mesmerize',\n",
       " 52058: 'quinnell',\n",
       " 10310: 'yahoo',\n",
       " 52060: 'meteorologist',\n",
       " 42580: 'beswick',\n",
       " 15496: 'boorman',\n",
       " 40850: 'voicework',\n",
       " 52061: \"ster'\",\n",
       " 22925: 'blustering',\n",
       " 52062: 'hj',\n",
       " 27637: 'intake',\n",
       " 5624: 'morally',\n",
       " 40852: 'jumbling',\n",
       " 52063: 'bowersock',\n",
       " 52064: \"'porky's'\",\n",
       " 16824: 'gershon',\n",
       " 40853: 'ludicrosity',\n",
       " 52065: 'coprophilia',\n",
       " 40854: 'expressively',\n",
       " 19503: \"india's\",\n",
       " 34713: \"post's\",\n",
       " 52066: 'wana',\n",
       " 5286: 'wang',\n",
       " 30574: 'wand',\n",
       " 25248: 'wane',\n",
       " 52324: 'edgeways',\n",
       " 34714: 'titanium',\n",
       " 40855: 'pinta',\n",
       " 181: 'want',\n",
       " 30575: 'pinto',\n",
       " 52068: 'whoopdedoodles',\n",
       " 21911: 'tchaikovsky',\n",
       " 2106: 'travel',\n",
       " 52069: \"'victory'\",\n",
       " 11931: 'copious',\n",
       " 22436: 'gouge',\n",
       " 52070: \"chapters'\",\n",
       " 6705: 'barbra',\n",
       " 30576: 'uselessness',\n",
       " 52071: \"wan'\",\n",
       " 27638: 'assimilated',\n",
       " 16119: 'petiot',\n",
       " 52072: 'most\\x85and',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 52073: 'seda',\n",
       " 52074: 'stollen',\n",
       " 34715: 'sentencing',\n",
       " 40856: 'ouroboros',\n",
       " 40857: 'assimilates',\n",
       " 40858: 'colorfully',\n",
       " 27639: 'glenne',\n",
       " 52075: 'dongen',\n",
       " 4763: 'subplots',\n",
       " 52076: 'kiloton',\n",
       " 23384: 'chandon',\n",
       " 34716: \"effect'\",\n",
       " 27640: 'snugly',\n",
       " 40859: 'kuei',\n",
       " 9095: 'welcomed',\n",
       " 30074: 'dishonor',\n",
       " 52078: 'concurrence',\n",
       " 23385: 'stoicism',\n",
       " 14899: \"guys'\",\n",
       " 52080: \"beroemd'\",\n",
       " 6706: 'butcher',\n",
       " 40860: \"melfi's\",\n",
       " 30626: 'aargh',\n",
       " 20602: 'playhouse',\n",
       " 11311: 'wickedly',\n",
       " 1183: 'fit',\n",
       " 52081: 'labratory',\n",
       " 40862: 'lifeline',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 52082: 'cineliterate',\n",
       " 52083: 'fic',\n",
       " 52084: 'fia',\n",
       " 34717: 'fig',\n",
       " 52085: 'fmvs',\n",
       " 52086: 'fie',\n",
       " 52087: 'reentered',\n",
       " 30577: 'fin',\n",
       " 52088: 'doctresses',\n",
       " 52089: 'fil',\n",
       " 12609: 'zucker',\n",
       " 31934: 'ached',\n",
       " 52091: 'counsil',\n",
       " 52092: 'paterfamilias',\n",
       " 13888: 'songwriter',\n",
       " 34718: 'shivam',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 52093: 'slauther',\n",
       " 52094: \"'flame'\",\n",
       " 52095: 'sommerset',\n",
       " 52096: 'interwhined',\n",
       " 27641: 'whacking',\n",
       " 52097: 'bartok',\n",
       " 8778: 'barton',\n",
       " 21912: 'frewer',\n",
       " 52098: \"fi'\",\n",
       " 6195: 'ingrid',\n",
       " 30578: 'stribor',\n",
       " 52099: 'approporiately',\n",
       " 52100: 'wobblyhand',\n",
       " 52101: 'tantalisingly',\n",
       " 52102: 'ankylosaurus',\n",
       " 17637: 'parasites',\n",
       " 52103: 'childen',\n",
       " 52104: \"jenkins'\",\n",
       " 52105: 'metafiction',\n",
       " 17638: 'golem',\n",
       " 40863: 'indiscretion',\n",
       " 23386: \"reeves'\",\n",
       " 57784: \"inamorata's\",\n",
       " 52107: 'brittannica',\n",
       " 7919: 'adapt',\n",
       " 30579: \"russo's\",\n",
       " 48249: 'guitarists',\n",
       " 10556: 'abbott',\n",
       " 40864: 'abbots',\n",
       " 17652: 'lanisha',\n",
       " 40866: 'magickal',\n",
       " 52108: 'mattter',\n",
       " 52109: \"'willy\",\n",
       " 34719: 'pumpkins',\n",
       " 52110: 'stuntpeople',\n",
       " 30580: 'estimate',\n",
       " 40867: 'ugghhh',\n",
       " 11312: 'gameplay',\n",
       " 52111: \"wern't\",\n",
       " 40868: \"n'sync\",\n",
       " 16120: 'sickeningly',\n",
       " 40869: 'chiara',\n",
       " 4014: 'disturbed',\n",
       " 40870: 'portmanteau',\n",
       " 52112: 'ineffectively',\n",
       " 82146: \"duchonvey's\",\n",
       " 37522: \"nasty'\",\n",
       " 1288: 'purpose',\n",
       " 52115: 'lazers',\n",
       " 28108: 'lightened',\n",
       " 52116: 'kaliganj',\n",
       " 52117: 'popularism',\n",
       " 18514: \"damme's\",\n",
       " 30581: 'stylistics',\n",
       " 52118: 'mindgaming',\n",
       " 46452: 'spoilerish',\n",
       " 52120: \"'corny'\",\n",
       " 34721: 'boerner',\n",
       " 6795: 'olds',\n",
       " 52121: 'bakelite',\n",
       " 27642: 'renovated',\n",
       " 27643: 'forrester',\n",
       " 52122: \"lumiere's\",\n",
       " 52027: 'gaskets',\n",
       " 887: 'needed',\n",
       " 34722: 'smight',\n",
       " 1300: 'master',\n",
       " 25908: \"edie's\",\n",
       " 40871: 'seeber',\n",
       " 52123: 'hiya',\n",
       " 52124: 'fuzziness',\n",
       " 14900: 'genesis',\n",
       " 12610: 'rewards',\n",
       " 30582: 'enthrall',\n",
       " 40872: \"'about\",\n",
       " 52125: \"recollection's\",\n",
       " 11042: 'mutilated',\n",
       " 52126: 'fatherlands',\n",
       " 52127: \"fischer's\",\n",
       " 5402: 'positively',\n",
       " 34708: '270',\n",
       " 34723: 'ahmed',\n",
       " 9839: 'zatoichi',\n",
       " 13889: 'bannister',\n",
       " 52130: 'anniversaries',\n",
       " 30583: \"helm's\",\n",
       " 52131: \"'work'\",\n",
       " 34724: 'exclaimed',\n",
       " 52132: \"'unfunny'\",\n",
       " 52032: '274',\n",
       " 547: 'feeling',\n",
       " 52134: \"wanda's\",\n",
       " 33269: 'dolan',\n",
       " 52136: '278',\n",
       " 52137: 'peacoat',\n",
       " 40873: 'brawny',\n",
       " 40874: 'mishra',\n",
       " 40875: 'worlders',\n",
       " 52138: 'protags',\n",
       " 52139: 'skullcap',\n",
       " 57599: 'dastagir',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 52140: 'hymen',\n",
       " 25249: 'paramedics',\n",
       " 52141: 'unpersons',\n",
       " 52142: 'heavyarms',\n",
       " 52143: 'affaire',\n",
       " 52144: 'coulisses',\n",
       " 40876: 'hymer',\n",
       " 52145: 'kremlin',\n",
       " 30584: 'shipments',\n",
       " 52146: 'pixilated',\n",
       " 30585: \"'00s\",\n",
       " 18515: 'diminishing',\n",
       " 1360: 'cinematic',\n",
       " 14901: 'resonates',\n",
       " 40877: 'simplify',\n",
       " 40878: \"nature'\",\n",
       " 40879: 'temptresses',\n",
       " 16825: 'reverence',\n",
       " 19505: 'resonated',\n",
       " 34725: 'dailey',\n",
       " 52147: '2\\x85',\n",
       " 27644: 'treize',\n",
       " 52148: 'majo',\n",
       " 21913: 'kiya',\n",
       " 52149: 'woolnough',\n",
       " 39800: 'thanatos',\n",
       " 35734: 'sandoval',\n",
       " 40882: 'dorama',\n",
       " 52150: \"o'shaughnessy\",\n",
       " 4991: 'tech',\n",
       " 32021: 'fugitives',\n",
       " 30586: 'teck',\n",
       " 76128: \"'e'\",\n",
       " 40884: 'doesn‚Äôt',\n",
       " 52152: 'purged',\n",
       " 660: 'saying',\n",
       " 41098: \"martians'\",\n",
       " 23421: 'norliss',\n",
       " 27645: 'dickey',\n",
       " 52155: 'dicker',\n",
       " 52156: \"'sependipity\",\n",
       " 8425: 'padded',\n",
       " 57795: 'ordell',\n",
       " 40885: \"sturges'\",\n",
       " 52157: 'independentcritics',\n",
       " 5748: 'tempted',\n",
       " 34727: \"atkinson's\",\n",
       " 25250: 'hounded',\n",
       " 52158: 'apace',\n",
       " 15497: 'clicked',\n",
       " 30587: \"'humor'\",\n",
       " 17180: \"martino's\",\n",
       " 52159: \"'supporting\",\n",
       " 52035: 'warmongering',\n",
       " 34728: \"zemeckis's\",\n",
       " 21914: 'lube',\n",
       " 52160: 'shocky',\n",
       " 7479: 'plate',\n",
       " 40886: 'plata',\n",
       " 40887: 'sturgess',\n",
       " 40888: \"nerds'\",\n",
       " 20603: 'plato',\n",
       " 34729: 'plath',\n",
       " 40889: 'platt',\n",
       " 52162: 'mcnab',\n",
       " 27646: 'clumsiness',\n",
       " 3902: 'altogether',\n",
       " 42587: 'massacring',\n",
       " 52163: 'bicenntinial',\n",
       " 40890: 'skaal',\n",
       " 14363: 'droning',\n",
       " 8779: 'lds',\n",
       " 21915: 'jaguar',\n",
       " 34730: \"cale's\",\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 18516: \"lot's\",\n",
       " 10089: 'patch',\n",
       " 50205: 'kerkhof',\n",
       " 52164: \"leader's\",\n",
       " 27647: \"'movie\",\n",
       " 52165: 'uncomfirmed',\n",
       " 40891: 'heirloom',\n",
       " 47363: 'wrangle',\n",
       " 52166: 'emotion\\x85',\n",
       " 52167: \"'stargate'\",\n",
       " 40892: 'pinoy',\n",
       " 40893: 'conchatta',\n",
       " 41131: 'broeke',\n",
       " 40894: 'advisedly',\n",
       " 17639: \"barker's\",\n",
       " 52169: 'descours',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 52170: 'lott',\n",
       " 40895: 'xvi',\n",
       " 34731: 'irk',\n",
       " 52171: 'irl',\n",
       " 6890: 'ira',\n",
       " 21916: 'belzer',\n",
       " 52172: 'irc',\n",
       " 27648: 'ire',\n",
       " 40896: 'requisites',\n",
       " 7696: 'discipline',\n",
       " 52964: 'lyoko',\n",
       " 11313: 'extend',\n",
       " 876: 'nature',\n",
       " 52173: \"'dickie'\",\n",
       " 40897: 'optimist',\n",
       " 30589: 'lapping',\n",
       " 3903: 'superficial',\n",
       " 52174: 'vestment',\n",
       " 2826: 'extent',\n",
       " 52175: 'tendons',\n",
       " 52176: \"heller's\",\n",
       " 52177: 'quagmires',\n",
       " 52178: 'miyako',\n",
       " 20604: 'moocow',\n",
       " 52179: \"coles'\",\n",
       " 40898: 'lookit',\n",
       " 52180: 'ravenously',\n",
       " 40899: 'levitating',\n",
       " 52181: 'perfunctorily',\n",
       " 30590: 'lookin',\n",
       " 40901: \"lot'\",\n",
       " 52182: 'lookie',\n",
       " 34873: 'fearlessly',\n",
       " 52184: 'libyan',\n",
       " 40902: 'fondles',\n",
       " 35717: 'gopher',\n",
       " 40904: 'wearying',\n",
       " 52185: \"nz's\",\n",
       " 27649: 'minuses',\n",
       " 52186: 'puposelessly',\n",
       " 52187: 'shandling',\n",
       " 31271: 'decapitates',\n",
       " 11932: 'humming',\n",
       " 40905: \"'nother\",\n",
       " 21917: 'smackdown',\n",
       " 30591: 'underdone',\n",
       " 40906: 'frf',\n",
       " 52188: 'triviality',\n",
       " 25251: 'fro',\n",
       " 8780: 'bothers',\n",
       " 52189: \"'kensington\",\n",
       " 76: 'much',\n",
       " 34733: 'muco',\n",
       " 22618: 'wiseguy',\n",
       " 27651: \"richie's\",\n",
       " 40907: 'tonino',\n",
       " 52190: 'unleavened',\n",
       " 11590: 'fry',\n",
       " 40908: \"'tv'\",\n",
       " 40909: 'toning',\n",
       " 14364: 'obese',\n",
       " 30592: 'sensationalized',\n",
       " 40910: 'spiv',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 21918: 'charleton',\n",
       " 16826: 'jeon',\n",
       " 21919: 'boardroom',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 53086: 'hepo',\n",
       " 27652: 'wildcat',\n",
       " 10587: 'venoms',\n",
       " 52194: 'misconstrues',\n",
       " 18517: 'mesmerising',\n",
       " 40911: 'misconstrued',\n",
       " 52195: 'rescinds',\n",
       " 52196: 'prostrate',\n",
       " 40912: 'majid',\n",
       " 16482: 'climbed',\n",
       " 34734: 'canoeing',\n",
       " 52198: 'majin',\n",
       " 57807: 'animie',\n",
       " 40913: 'sylke',\n",
       " 14902: 'conditioned',\n",
       " 40914: 'waddell',\n",
       " 52199: '3\\x85',\n",
       " 41191: 'hyperdrive',\n",
       " 34735: 'conditioner',\n",
       " 53156: 'bricklayer',\n",
       " 2579: 'hong',\n",
       " 52201: 'memoriam',\n",
       " 30595: 'inventively',\n",
       " 25252: \"levant's\",\n",
       " 20641: 'portobello',\n",
       " 52203: 'remand',\n",
       " 19507: 'mummified',\n",
       " 27653: 'honk',\n",
       " 19508: 'spews',\n",
       " 40915: 'visitations',\n",
       " 52204: 'mummifies',\n",
       " 25253: 'cavanaugh',\n",
       " 23388: 'zeon',\n",
       " 40916: \"jungle's\",\n",
       " 34736: 'viertel',\n",
       " 27654: 'frenchmen',\n",
       " 52205: 'torpedoes',\n",
       " 52206: 'schlessinger',\n",
       " 34737: 'torpedoed',\n",
       " 69879: 'blister',\n",
       " 52207: 'cinefest',\n",
       " 34738: 'furlough',\n",
       " 52208: 'mainsequence',\n",
       " 40917: 'mentors',\n",
       " 9097: 'academic',\n",
       " 20605: 'stillness',\n",
       " 40918: 'academia',\n",
       " 52209: 'lonelier',\n",
       " 52210: 'nibby',\n",
       " 52211: \"losers'\",\n",
       " 40919: 'cineastes',\n",
       " 4452: 'corporate',\n",
       " 40920: 'massaging',\n",
       " 30596: 'bellow',\n",
       " 19509: 'absurdities',\n",
       " 53244: 'expetations',\n",
       " 40921: 'nyfiken',\n",
       " 75641: 'mehras',\n",
       " 52212: 'lasse',\n",
       " 52213: 'visability',\n",
       " 33949: 'militarily',\n",
       " 52214: \"elder'\",\n",
       " 19026: 'gainsbourg',\n",
       " 20606: 'hah',\n",
       " 13423: 'hai',\n",
       " 34739: 'haj',\n",
       " 25254: 'hak',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 53262: 'duffer',\n",
       " 52216: 'haa',\n",
       " 69: 'had',\n",
       " 11933: 'advancement',\n",
       " 16828: 'hag',\n",
       " 25255: \"hand'\",\n",
       " 13424: 'hay',\n",
       " 20607: 'mcnamara',\n",
       " 52217: \"mozart's\",\n",
       " 30734: 'duffel',\n",
       " 30597: 'haq',\n",
       " 13890: 'har',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 40922: 'hav',\n",
       " 30598: 'haw',\n",
       " 52218: 'figtings',\n",
       " 15498: 'elders',\n",
       " 52219: 'underpanted',\n",
       " 52220: 'pninson',\n",
       " 27655: 'unequivocally',\n",
       " 23676: \"barbara's\",\n",
       " 52222: \"bello'\",\n",
       " 13000: 'indicative',\n",
       " 40923: 'yawnfest',\n",
       " 52223: 'hexploitation',\n",
       " 52224: \"loder's\",\n",
       " 27656: 'sleuthing',\n",
       " 32625: \"justin's\",\n",
       " 52225: \"'ball\",\n",
       " 52226: \"'summer\",\n",
       " 34938: \"'demons'\",\n",
       " 52228: \"mormon's\",\n",
       " 34740: \"laughton's\",\n",
       " 52229: 'debell',\n",
       " 39727: 'shipyard',\n",
       " 30600: 'unabashedly',\n",
       " 40404: 'disks',\n",
       " 2293: 'crowd',\n",
       " 10090: 'crowe',\n",
       " 56437: \"vancouver's\",\n",
       " 34741: 'mosques',\n",
       " 6630: 'crown',\n",
       " 52230: 'culpas',\n",
       " 27657: 'crows',\n",
       " 53347: 'surrell',\n",
       " 52232: 'flowless',\n",
       " 52233: 'sheirk',\n",
       " 40926: \"'three\",\n",
       " 52234: \"peterson'\",\n",
       " 52235: 'ooverall',\n",
       " 40927: 'perchance',\n",
       " 1324: 'bottom',\n",
       " 53366: 'chabert',\n",
       " 52236: 'sneha',\n",
       " 13891: 'inhuman',\n",
       " 52237: 'ichii',\n",
       " 52238: 'ursla',\n",
       " 30601: 'completly',\n",
       " 40928: 'moviedom',\n",
       " 52239: 'raddick',\n",
       " 51998: 'brundage',\n",
       " 40929: 'brigades',\n",
       " 1184: 'starring',\n",
       " 52240: \"'goal'\",\n",
       " 52241: 'caskets',\n",
       " 52242: 'willcock',\n",
       " 52243: \"threesome's\",\n",
       " 52244: \"mosque'\",\n",
       " 52245: \"cover's\",\n",
       " 17640: 'spaceships',\n",
       " 40930: 'anomalous',\n",
       " 27658: 'ptsd',\n",
       " 52246: 'shirdan',\n",
       " 21965: 'obscenity',\n",
       " 30602: 'lemmings',\n",
       " 30603: 'duccio',\n",
       " 52247: \"levene's\",\n",
       " 52248: \"'gorby'\",\n",
       " 25258: \"teenager's\",\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 12261: 'despised',\n",
       " 52249: 'okabasho',\n",
       " 8292: 'fabric',\n",
       " 18518: 'cannavale',\n",
       " 3540: 'raped',\n",
       " 52250: \"tutt's\",\n",
       " 17641: 'grasping',\n",
       " 18519: 'despises',\n",
       " 40931: \"thief's\",\n",
       " 8929: 'rapes',\n",
       " 52251: 'raper',\n",
       " 27659: \"eyre'\",\n",
       " 52252: 'walchek',\n",
       " 23389: \"elmo's\",\n",
       " 40932: 'perfumes',\n",
       " 21921: 'spurting',\n",
       " 52253: \"exposition'\\x85\",\n",
       " 52254: 'denoting',\n",
       " 34743: 'thesaurus',\n",
       " 40933: \"shoot'\",\n",
       " 49762: 'bonejack',\n",
       " 52256: 'simpsonian',\n",
       " 30604: 'hebetude',\n",
       " 34744: \"hallow's\",\n",
       " 52257: 'desperation\\x85',\n",
       " 34745: 'incinerator',\n",
       " 10311: 'congratulations',\n",
       " 52258: 'humbled',\n",
       " 5927: \"else's\",\n",
       " 40848: 'trelkovski',\n",
       " 52259: \"rape'\",\n",
       " 59389: \"'chapters'\",\n",
       " 52260: '1600s',\n",
       " 7256: 'martian',\n",
       " 25259: 'nicest',\n",
       " 52262: 'eyred',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 52263: 'moderne',\n",
       " 5123: 'barrymore',\n",
       " 52264: 'yankovich',\n",
       " 40934: 'moderns',\n",
       " 52265: 'studliest',\n",
       " 52266: 'bedsheet',\n",
       " 14903: 'decapitation',\n",
       " 52267: 'slurring',\n",
       " 52268: \"'nunsploitation'\",\n",
       " 34746: \"'character'\",\n",
       " 9883: 'cambodia',\n",
       " 52269: 'rebelious',\n",
       " 27660: 'pasadena',\n",
       " 40935: 'crowne',\n",
       " 52270: \"'bedchamber\",\n",
       " 52271: 'conjectural',\n",
       " 52272: 'appologize',\n",
       " 52273: 'halfassing',\n",
       " 57819: 'paycheque',\n",
       " 20609: 'palms',\n",
       " 52274: \"'islands\",\n",
       " 40936: 'hawked',\n",
       " 21922: 'palme',\n",
       " 40937: 'conservatively',\n",
       " 64010: 'larp',\n",
       " 5561: 'palma',\n",
       " 21923: 'smelling',\n",
       " 13001: 'aragorn',\n",
       " 52275: 'hawker',\n",
       " 52276: 'hawkes',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 52277: \"pyle's\",\n",
       " 6707: 'shootout',\n",
       " 18520: \"mike's\",\n",
       " 52278: \"driscoll's\",\n",
       " 40938: 'cogsworth',\n",
       " 52279: \"britian's\",\n",
       " 34747: 'childs',\n",
       " 52280: \"portrait's\",\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 52281: 'puttered',\n",
       " 52282: 'childe',\n",
       " 52283: 'maywether',\n",
       " 3039: 'chair',\n",
       " 52284: \"rance's\",\n",
       " 34748: 'machu',\n",
       " 4520: 'ballet',\n",
       " 34749: 'grapples',\n",
       " 76155: 'summerize',\n",
       " 30606: 'freelance',\n",
       " 52286: \"andrea's\",\n",
       " 52287: '\\x91very',\n",
       " 45882: 'coolidge',\n",
       " 18521: 'mache',\n",
       " 52288: 'balled',\n",
       " 40940: 'grappled',\n",
       " 18522: 'macha',\n",
       " 21924: 'underlining',\n",
       " 5626: 'macho',\n",
       " 19510: 'oversight',\n",
       " 25260: 'machi',\n",
       " 11314: 'verbally',\n",
       " 21925: 'tenacious',\n",
       " 40941: 'windshields',\n",
       " 18560: 'paychecks',\n",
       " 3399: 'jerk',\n",
       " 11934: \"good'\",\n",
       " 34751: 'prancer',\n",
       " 21926: 'prances',\n",
       " 52289: 'olympus',\n",
       " 21927: 'lark',\n",
       " 10788: 'embark',\n",
       " 7368: 'gloomy',\n",
       " 52290: 'jehaan',\n",
       " 52291: 'turaqui',\n",
       " 20610: \"child'\",\n",
       " 2897: 'locked',\n",
       " 52292: 'pranced',\n",
       " 2591: 'exact',\n",
       " 52293: 'unattuned',\n",
       " 786: 'minute',\n",
       " 16121: 'skewed',\n",
       " 40943: 'hodgins',\n",
       " 34752: 'skewer',\n",
       " 52294: 'think\\x85',\n",
       " 38768: 'rosenstein',\n",
       " 52295: 'helmit',\n",
       " 34753: 'wrestlemanias',\n",
       " 16829: 'hindered',\n",
       " 30607: \"martha's\",\n",
       " 52296: 'cheree',\n",
       " 52297: \"pluckin'\",\n",
       " 40944: 'ogles',\n",
       " 11935: 'heavyweight',\n",
       " 82193: 'aada',\n",
       " 11315: 'chopping',\n",
       " 61537: 'strongboy',\n",
       " 41345: 'hegemonic',\n",
       " 40945: 'adorns',\n",
       " 41349: 'xxth',\n",
       " 34754: 'nobuhiro',\n",
       " 52301: 'capit√£es',\n",
       " 52302: 'kavogianni',\n",
       " 13425: 'antwerp',\n",
       " 6541: 'celebrated',\n",
       " 52303: 'roarke',\n",
       " 40946: 'baggins',\n",
       " 31273: 'cheeseburgers',\n",
       " 52304: 'matras',\n",
       " 52305: \"nineties'\",\n",
       " 52306: \"'craig'\",\n",
       " 13002: 'celebrates',\n",
       " 3386: 'unintentionally',\n",
       " 14365: 'drafted',\n",
       " 52307: 'climby',\n",
       " 52308: '303',\n",
       " 18523: 'oldies',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 34755: 'plucking',\n",
       " 30077: '305',\n",
       " 5517: 'address',\n",
       " 40947: 'menjou',\n",
       " 42595: \"'freak'\",\n",
       " 19511: 'dwindling',\n",
       " 9461: 'benson',\n",
       " 52310: 'white‚Äôs',\n",
       " 40948: 'shamelessness',\n",
       " 21928: 'impacted',\n",
       " 52311: 'upatz',\n",
       " 3843: 'cusack',\n",
       " 37570: \"flavia's\",\n",
       " 52312: 'effette',\n",
       " 34756: 'influx',\n",
       " 52313: 'boooooooo',\n",
       " 52314: 'dimitrova',\n",
       " 13426: 'houseman',\n",
       " 25262: 'bigas',\n",
       " 52315: 'boylen',\n",
       " 52316: 'phillipenes',\n",
       " 40949: 'fakery',\n",
       " 27661: \"grandpa's\",\n",
       " 27662: 'darnell',\n",
       " 19512: 'undergone',\n",
       " 52318: 'handbags',\n",
       " 21929: 'perished',\n",
       " 37781: 'pooped',\n",
       " 27663: 'vigour',\n",
       " 3630: 'opposed',\n",
       " 52319: 'etude',\n",
       " 11802: \"caine's\",\n",
       " 52320: 'doozers',\n",
       " 34757: 'photojournals',\n",
       " 52321: 'perishes',\n",
       " 34758: 'constrains',\n",
       " 40951: 'migenes',\n",
       " 30608: 'consoled',\n",
       " 16830: 'alastair',\n",
       " 52322: 'wvs',\n",
       " 52323: 'ooooooh',\n",
       " 34759: 'approving',\n",
       " 40952: 'consoles',\n",
       " 52067: 'disparagement',\n",
       " 52325: 'futureistic',\n",
       " 52326: 'rebounding',\n",
       " 52327: \"'date\",\n",
       " 52328: 'gregoire',\n",
       " 21930: 'rutherford',\n",
       " 34760: 'americanised',\n",
       " 82199: 'novikov',\n",
       " 1045: 'following',\n",
       " 34761: 'munroe',\n",
       " 52329: \"morita'\",\n",
       " 52330: 'christenssen',\n",
       " 23109: 'oatmeal',\n",
       " 25263: 'fossey',\n",
       " 40953: 'livered',\n",
       " 13003: 'listens',\n",
       " 76167: \"'marci\",\n",
       " 52333: \"otis's\",\n",
       " 23390: 'thanking',\n",
       " 16022: 'maude',\n",
       " 34762: 'extensions',\n",
       " 52335: 'ameteurish',\n",
       " 52336: \"commender's\",\n",
       " 27664: 'agricultural',\n",
       " 4521: 'convincingly',\n",
       " 17642: 'fueled',\n",
       " 54017: 'mahattan',\n",
       " 40955: \"paris's\",\n",
       " 52339: 'vulkan',\n",
       " 52340: 'stapes',\n",
       " 52341: 'odysessy',\n",
       " 12262: 'harmon',\n",
       " 4255: 'surfing',\n",
       " 23497: 'halloran',\n",
       " 49583: 'unbelieveably',\n",
       " 52342: \"'offed'\",\n",
       " 30610: 'quadrant',\n",
       " 19513: 'inhabiting',\n",
       " 34763: 'nebbish',\n",
       " 40956: 'forebears',\n",
       " 34764: 'skirmish',\n",
       " 52343: 'ocassionally',\n",
       " 52344: \"'resist\",\n",
       " 21931: 'impactful',\n",
       " 52345: 'spicier',\n",
       " 40957: 'touristy',\n",
       " 52346: \"'football'\",\n",
       " 40958: 'webpage',\n",
       " 52348: 'exurbia',\n",
       " 52349: 'jucier',\n",
       " 14904: 'professors',\n",
       " 34765: 'structuring',\n",
       " 30611: 'jig',\n",
       " 40959: 'overlord',\n",
       " 25264: 'disconnect',\n",
       " 82204: 'sniffle',\n",
       " 40960: 'slimeball',\n",
       " 40961: 'jia',\n",
       " 16831: 'milked',\n",
       " 40962: 'banjoes',\n",
       " 1240: 'jim',\n",
       " 52351: 'workforces',\n",
       " 52352: 'jip',\n",
       " 52353: 'rotweiller',\n",
       " 34766: 'mundaneness',\n",
       " 52354: \"'ninja'\",\n",
       " 11043: \"dead'\",\n",
       " 40963: \"cipriani's\",\n",
       " 20611: 'modestly',\n",
       " 52355: \"professor'\",\n",
       " 40964: 'shacked',\n",
       " 34767: 'bashful',\n",
       " 23391: 'sorter',\n",
       " 16123: 'overpowering',\n",
       " 18524: 'workmanlike',\n",
       " 27665: 'henpecked',\n",
       " 18525: 'sorted',\n",
       " 52357: \"j≈çb's\",\n",
       " 52358: \"'always\",\n",
       " 34768: \"'baptists\",\n",
       " 52359: 'dreamcatchers',\n",
       " 52360: \"'silence'\",\n",
       " 21932: 'hickory',\n",
       " 52361: 'fun\\x97yet',\n",
       " 52362: 'breakumentary',\n",
       " 15499: 'didn',\n",
       " 52363: 'didi',\n",
       " 52364: 'pealing',\n",
       " 40965: 'dispite',\n",
       " 25265: \"italy's\",\n",
       " 21933: 'instability',\n",
       " 6542: 'quarter',\n",
       " 12611: 'quartet',\n",
       " 52365: 'padm√©',\n",
       " 52366: \"'bleedmedry\",\n",
       " 52367: 'pahalniuk',\n",
       " 52368: 'honduras',\n",
       " 10789: 'bursting',\n",
       " 41468: \"pablo's\",\n",
       " 52370: 'irremediably',\n",
       " 40966: 'presages',\n",
       " 57835: 'bowlegged',\n",
       " 65186: 'dalip',\n",
       " 6263: 'entering',\n",
       " 76175: 'newsradio',\n",
       " 54153: 'presaged',\n",
       " 27666: \"giallo's\",\n",
       " 40967: 'bouyant',\n",
       " 52371: 'amerterish',\n",
       " 18526: 'rajni',\n",
       " 30613: 'leeves',\n",
       " 34770: 'macauley',\n",
       " 615: 'seriously',\n",
       " 52372: 'sugercoma',\n",
       " 52373: 'grimstead',\n",
       " 52374: \"'fairy'\",\n",
       " 30614: 'zenda',\n",
       " 52375: \"'twins'\",\n",
       " 17643: 'realisation',\n",
       " 27667: 'highsmith',\n",
       " 7820: 'raunchy',\n",
       " 40968: 'incentives',\n",
       " 52377: 'flatson',\n",
       " 35100: 'snooker',\n",
       " 16832: 'crazies',\n",
       " 14905: 'crazier',\n",
       " 7097: 'grandma',\n",
       " 52378: 'napunsaktha',\n",
       " 30615: 'workmanship',\n",
       " 52379: 'reisner',\n",
       " 61309: \"sanford's\",\n",
       " 52380: '\\x91do√±a',\n",
       " 6111: 'modest',\n",
       " 19156: \"everything's\",\n",
       " 40969: 'hamer',\n",
       " 52382: \"couldn't'\",\n",
       " 13004: 'quibble',\n",
       " 52383: 'socking',\n",
       " 21934: 'tingler',\n",
       " 52384: 'gutman',\n",
       " 40970: 'lachlan',\n",
       " 52385: 'tableaus',\n",
       " 52386: 'headbanger',\n",
       " 2850: 'spoken',\n",
       " 34771: 'cerebrally',\n",
       " 23493: \"'road\",\n",
       " 21935: 'tableaux',\n",
       " 40971: \"proust's\",\n",
       " 40972: 'periodical',\n",
       " 52388: \"shoveller's\",\n",
       " 25266: 'tamara',\n",
       " 17644: 'affords',\n",
       " 3252: 'concert',\n",
       " 87958: \"yara's\",\n",
       " 52389: 'someome',\n",
       " 8427: 'lingering',\n",
       " 41514: \"abraham's\",\n",
       " 34772: 'beesley',\n",
       " 34773: 'cherbourg',\n",
       " 28627: 'kagan',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 25267: 'absorbs',\n",
       " 40973: \"koltai's\",\n",
       " 64030: 'tingled',\n",
       " 19514: 'crossroads',\n",
       " 16124: 'rehab',\n",
       " 52392: 'falworth',\n",
       " 52393: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6,  346,  137,   11,    4, 2768,  295,   36, 7740,  725,    6,\n",
       "       3208,  273,   11,    4, 1513,   15, 1367,   35,  154,    2,  103,\n",
       "          2,  173,    7,   12,   36,  515, 3547,   94, 2547, 1722,    5,\n",
       "       3547,   36,  203,   30,  502,    8,  361,   12,    8,  989,  143,\n",
       "          4, 1172, 3404,   10,   10,  328, 1236,    9,    6,   55,  221,\n",
       "       2989,    5,  146,  165,  179,  770,   15,   50,  713,   53,  108,\n",
       "        448,   23,   12,   17,  225,   38,   76, 4397,   18,  183,    8,\n",
       "         81,   19,   12,   45, 1257,    8,  135,   15,    2,  166,    4,\n",
       "        118,    7,   45,    2,   17,  466,   45,    2,    4,   22,  115,\n",
       "        165,  764, 6075,    5, 1030,    8, 2973,   73,  469,  167, 2127,\n",
       "          2, 1568,    6,   87,  841,   18,    4,   22,    4,  192,   15,\n",
       "         91,    7,   12,  304,  273, 1004,    4, 1375, 1172, 2768,    2,\n",
       "         15,    4,   22,  764,   55, 5773,    5,   14, 4233, 7444,    4,\n",
       "       1375,  326,    7,    4, 4760, 1786,    8,  361, 1236,    8,  989,\n",
       "         46,    7,    4, 2768,   45,   55,  776,    8,   79,  496,   98,\n",
       "         45,  400,  301,   15,    4, 1859,    9,    4,  155,   15,   66,\n",
       "          2,   84,    5,   14,   22, 1534,   15,   17,    4,  167,    2,\n",
       "         15,   75,   70,  115,   66,   30,  252,    7,  618,   51,    9,\n",
       "       2161,    4, 3130,    5,   14, 1525,    8, 6584,   15,    2,  165,\n",
       "        127, 1921,    8,   30,  179, 2532,    4,   22,    9,  906,   18,\n",
       "          6,  176,    7, 1007, 1005,    4, 1375,  114,    4,  105,   26,\n",
       "         32,   55,  221,   11,   68,  205,   96,    5,    4,  192,   15,\n",
       "          4,  274,  410,  220,  304,   23,   94,  205,  109,    9,   55,\n",
       "         73,  224,  259, 3786,   15,    4,   22,  528, 1645,   34,    4,\n",
       "        130,  528,   30,  685,  345,   17,    4,  277,  199,  166,  281,\n",
       "          5, 1030,    8,   30,  179, 4442,  444,    2,    9,    6,  371,\n",
       "         87,  189,   22,    5,   31,    7,    4,  118,    7,    4, 2068,\n",
       "        545, 1178,  829])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets Decode and check for some train values.\n",
    "x_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a short while in the cell together they stumble upon a hiding place in the wall that contains an old [‚ùì] after [‚ùì] part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that [‚ùì] makes the best of it's [‚ùì] as despite it's [‚ùì] the film never actually feels restrained and manages to flow well throughout director eric [‚ùì] provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell [‚ùì] that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really [‚ùì] people and this film proves that as the director [‚ùì] that we can never really be sure of exactly what is round the corner and this helps to ensure that [‚ùì] actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall [‚ùì] is a truly great horror film and one of the best of the decade highly recommended viewing\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_original_text(x_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    1,  591,  202,   14,   31,    6,  717,   10,   10,    2,\n",
       "          2,    5,    4,  360,    7,    4,  177, 5760,  394,  354,    4,\n",
       "        123,    9, 1035, 1035, 1035,   10,   10,   13,   92,  124,   89,\n",
       "        488, 7944,  100,   28, 1668,   14,   31,   23,   27, 7479,   29,\n",
       "        220,  468,    8,  124,   14,  286,  170,    8,  157,   46,    5,\n",
       "         27,  239,   16,  179,    2,   38,   32,   25, 7944,  451,  202,\n",
       "         14,    6,  717])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets Decode and check for some test values.\n",
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [üèÉ] please give this one a miss br br [‚ùì] [‚ùì] and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite [‚ùì] so all you madison fans give this a miss\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_original_text(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 163,713\n",
      "Trainable params: 163,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "#from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional, GlobalAveragePooling1D\n",
    "\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iyyappan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 9s 362us/step - loss: 0.6959 - acc: 0.4995 - val_loss: 0.6927 - val_acc: 0.5039\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 7s 267us/step - loss: 0.6927 - acc: 0.5141 - val_loss: 0.6913 - val_acc: 0.5001\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 5s 205us/step - loss: 0.6900 - acc: 0.5336 - val_loss: 0.6850 - val_acc: 0.5682\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 7s 267us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.6557 - val_acc: 0.6728\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 8s 310us/step - loss: 0.6163 - acc: 0.6857 - val_loss: 0.5369 - val_acc: 0.7860\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 12s 465us/step - loss: 0.4822 - acc: 0.7855 - val_loss: 0.4075 - val_acc: 0.8319\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 8s 333us/step - loss: 0.3810 - acc: 0.8429 - val_loss: 0.3500 - val_acc: 0.8539\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.3277 - acc: 0.8680 - val_loss: 0.3238 - val_acc: 0.8637s - loss: 0.3358 \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.2946 - acc: 0.8853 - val_loss: 0.3104 - val_acc: 0.8704\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.2738 - acc: 0.8951 - val_loss: 0.3007 - val_acc: 0.8735\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 119us/step - loss: 0.2564 - acc: 0.9026 - val_loss: 0.3037 - val_acc: 0.8735\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.2395 - acc: 0.9100 - val_loss: 0.2951 - val_acc: 0.8763\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.2266 - acc: 0.9138 - val_loss: 0.2925 - val_acc: 0.8803\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 134us/step - loss: 0.2122 - acc: 0.9224 - val_loss: 0.2944 - val_acc: 0.8806\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 124us/step - loss: 0.2025 - acc: 0.9268 - val_loss: 0.2986 - val_acc: 0.8819\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 134us/step - loss: 0.1950 - acc: 0.9291 - val_loss: 0.2941 - val_acc: 0.8810\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 138us/step - loss: 0.1866 - acc: 0.9342 - val_loss: 0.3015 - val_acc: 0.8812\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 122us/step - loss: 0.1756 - acc: 0.9386 - val_loss: 0.3053 - val_acc: 0.8804\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 134us/step - loss: 0.1683 - acc: 0.9410 - val_loss: 0.3032 - val_acc: 0.8812\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.1683 - acc: 0.9411 - val_loss: 0.3234 - val_acc: 0.8764\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Report the Accuracy of the model. (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance: Log Loss and Accuracy on train data\n",
      "25000/25000 [==============================] - 3s 110us/step\n",
      "[0.1387550181351602, 0.9538399930953979]\n",
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "25000/25000 [==============================] - 3s 135us/step\n",
      "[0.3233553539603949, 0.8763999971866607]\n"
     ]
    }
   ],
   "source": [
    "print('\\nModel Performance: Log Loss and Accuracy on train data')\n",
    "print(model.evaluate(x_train, y_train, batch_size = 20))\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "print(model.evaluate(x_test, y_test, batch_size = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right :  95\n",
      "mistake :  5\n",
      "accuracy: 0.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>x_test_original_text</th>\n",
       "      <th>probability</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>y_test</th>\n",
       "      <th>is_fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16001</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.950951</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2593</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13786</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.014839</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3038</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.987743</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15828</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.020465</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14472</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.823340</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2479</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12321</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14374</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.020176</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>334</td>\n",
       "      <td>several town members she [‚ùì] as though she had...</td>\n",
       "      <td>0.028556</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22569</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.953812</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>687</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.900597</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24752</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.991578</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5449</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11699</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.955848</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9810</td>\n",
       "      <td>of the movie's strongest scenes as she's re in...</td>\n",
       "      <td>0.725022</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20884</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.102249</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7786</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7703</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.074411</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>240</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24857</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.108662</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>238</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.012239</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1098</td>\n",
       "      <td>a straight face br br the director [‚ùì] every [...</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12907</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.130395</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14006</td>\n",
       "      <td>of course is largely responsible for the arres...</td>\n",
       "      <td>0.978992</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6906</td>\n",
       "      <td>br br yes this is from [‚ùì] [‚ùì] from beyond goo...</td>\n",
       "      <td>0.048765</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18351</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.925773</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15557</td>\n",
       "      <td>[‚ùì] br br even though a bit strange at first t...</td>\n",
       "      <td>0.642099</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9890</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.989785</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23643</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.733643</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>16451</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.995937</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>23082</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.998801</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7496</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.998284</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>22242</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.998809</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>17416</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.996590</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>8954</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [üèÉ] mind n...</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23788</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>619</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.158225</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5556</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.310725</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7443</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.037018</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7596</td>\n",
       "      <td>work for the next 25 years in 1931 he had rece...</td>\n",
       "      <td>0.963508</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>10145</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.057697</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>226</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.969931</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4818</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>17489</td>\n",
       "      <td>herself almost in the exact same way br br [‚ùì]...</td>\n",
       "      <td>0.025202</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>22008</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7896</td>\n",
       "      <td>he would have long since seen though the decep...</td>\n",
       "      <td>0.599646</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>18904</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6275</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.133878</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>24173</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.987177</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>19285</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5183</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.418776</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>19181</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.922429</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>12636</td>\n",
       "      <td>of the 1950's original and about 20 minutes in...</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>14878</td>\n",
       "      <td>value of some of the more recent adaptations i...</td>\n",
       "      <td>0.994679</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>19706</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.049976</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>297</td>\n",
       "      <td>first have to [‚ùì] faith and trust in each othe...</td>\n",
       "      <td>0.114705</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3273</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.078438</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12031</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.119024</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12500</td>\n",
       "      <td>[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                               x_test_original_text  probability  \\\n",
       "0   16001  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.950951   \n",
       "1    2593  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.008440   \n",
       "2   13786  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.014839   \n",
       "3    3038  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.987743   \n",
       "4   15828  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.020465   \n",
       "5   14472  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.823340   \n",
       "6    2479  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.050895   \n",
       "7   12321  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.596800   \n",
       "8   14374  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.020176   \n",
       "9     334  several town members she [‚ùì] as though she had...     0.028556   \n",
       "10  22569  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.953812   \n",
       "11    687  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.900597   \n",
       "12  24752  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.991578   \n",
       "13   5449  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.975997   \n",
       "14  11699  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.955848   \n",
       "15   9810  of the movie's strongest scenes as she's re in...     0.725022   \n",
       "16  20884  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.102249   \n",
       "17   7786  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.024611   \n",
       "18   7703  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.074411   \n",
       "19    240  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.031112   \n",
       "20  24857  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.108662   \n",
       "21    238  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.012239   \n",
       "22   1098  a straight face br br the director [‚ùì] every [...     0.003021   \n",
       "23  12907  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.130395   \n",
       "24  14006  of course is largely responsible for the arres...     0.978992   \n",
       "25   6906  br br yes this is from [‚ùì] [‚ùì] from beyond goo...     0.048765   \n",
       "26  18351  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.925773   \n",
       "27  15557  [‚ùì] br br even though a bit strange at first t...     0.642099   \n",
       "28   9890  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.989785   \n",
       "29  23643  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.733643   \n",
       "..    ...                                                ...          ...   \n",
       "70  16451  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.995937   \n",
       "71  23082  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.998801   \n",
       "72   7496  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.998284   \n",
       "73  22242  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.998809   \n",
       "74  17416  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.996590   \n",
       "75   8954  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [üèÉ] mind n...     0.000691   \n",
       "76  23788  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.018476   \n",
       "77    619  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.158225   \n",
       "78   5556  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.310725   \n",
       "79   7443  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.037018   \n",
       "80   7596  work for the next 25 years in 1931 he had rece...     0.963508   \n",
       "81  10145  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.057697   \n",
       "82    226  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.969931   \n",
       "83   4818  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.997807   \n",
       "84  17489  herself almost in the exact same way br br [‚ùì]...     0.025202   \n",
       "85  22008  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.024625   \n",
       "86   7896  he would have long since seen though the decep...     0.599646   \n",
       "87  18904  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.946187   \n",
       "88   6275  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.133878   \n",
       "89  24173  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.987177   \n",
       "90  19285  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.008377   \n",
       "91   5183  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.418776   \n",
       "92  19181  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.922429   \n",
       "93  12636  of the 1950's original and about 20 minutes in...     0.030600   \n",
       "94  14878  value of some of the more recent adaptations i...     0.994679   \n",
       "95  19706  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.049976   \n",
       "96    297  first have to [‚ùì] faith and trust in each othe...     0.114705   \n",
       "97   3273  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.078438   \n",
       "98  12031  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.119024   \n",
       "99  12500  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD...     0.004692   \n",
       "\n",
       "   pred_class    y_test is_fail  \n",
       "0    positive  positive          \n",
       "1    negative  negative          \n",
       "2    negative  negative          \n",
       "3    positive  positive          \n",
       "4    negative  negative          \n",
       "5    positive  positive          \n",
       "6    negative  negative          \n",
       "7    positive  negative    Fail  \n",
       "8    negative  negative          \n",
       "9    negative  negative          \n",
       "10   positive  positive          \n",
       "11   positive  positive          \n",
       "12   positive  positive          \n",
       "13   positive  positive          \n",
       "14   positive  positive          \n",
       "15   positive  positive          \n",
       "16   negative  negative          \n",
       "17   negative  negative          \n",
       "18   negative  negative          \n",
       "19   negative  negative          \n",
       "20   negative  negative          \n",
       "21   negative  negative          \n",
       "22   negative  negative          \n",
       "23   negative  negative          \n",
       "24   positive  positive          \n",
       "25   negative  negative          \n",
       "26   positive  positive          \n",
       "27   positive  negative    Fail  \n",
       "28   positive  positive          \n",
       "29   positive  positive          \n",
       "..        ...       ...     ...  \n",
       "70   positive  positive          \n",
       "71   positive  positive          \n",
       "72   positive  positive          \n",
       "73   positive  positive          \n",
       "74   positive  positive          \n",
       "75   negative  negative          \n",
       "76   negative  negative          \n",
       "77   negative  negative          \n",
       "78   negative  negative          \n",
       "79   negative  negative          \n",
       "80   positive  positive          \n",
       "81   negative  negative          \n",
       "82   positive  positive          \n",
       "83   positive  positive          \n",
       "84   negative  negative          \n",
       "85   negative  negative          \n",
       "86   positive  positive          \n",
       "87   positive  positive          \n",
       "88   negative  negative          \n",
       "89   positive  positive          \n",
       "90   negative  negative          \n",
       "91   negative  positive    Fail  \n",
       "92   positive  positive          \n",
       "93   negative  negative          \n",
       "94   positive  positive          \n",
       "95   negative  negative          \n",
       "96   negative  negative          \n",
       "97   negative  negative          \n",
       "98   negative  positive    Fail  \n",
       "99   negative  negative          \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PREDICT\n",
    "'''\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "original_x_test = x_test\n",
    "\n",
    "def restore_original_text(imdb_x_array):\n",
    "    return (' '.join(id_to_word[id] for id in imdb_x_array ))\n",
    "\n",
    "def imdb_class_to_str(imdb_class):\n",
    "    if imdb_class == 0:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "right = 0\n",
    "mistake = 0\n",
    "\n",
    "index_list = []\n",
    "original_text_list = []\n",
    "pred_prob_list = []\n",
    "pred_class_list = []\n",
    "y_test_list = []\n",
    "fail_str_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    index = random.randint(0, len(x_test))\n",
    "    \n",
    "    pred_prob = model.predict(x_test[index:(index+1)])[0][0] \n",
    "    pred_class = model.predict_classes(x_test[index:(index+1)])[0][0]\n",
    "    \n",
    "    '''\n",
    "    print('pred_prod:', pred_prod)\n",
    "    print('pred_class:', pred_class)\n",
    "    print('y_test[index] :', y_test[index])\n",
    "    '''\n",
    "    fail_str = '' \n",
    "    \n",
    "    if y_test[index] == pred_class:\n",
    "        right += 1\n",
    "    else:\n",
    "        mistake += 1\n",
    "        fail_str = 'Fail'\n",
    "        \n",
    "    original_text = restore_original_text(original_x_test[index])\n",
    "\n",
    "    index_list.append(index)\n",
    "    original_text_list.append(original_text)\n",
    "    pred_prob_list.append(pred_prob)\n",
    "    pred_class_list.append(imdb_class_to_str(pred_class))\n",
    "    y_test_list.append(imdb_class_to_str(y_test[index]))\n",
    "    fail_str_list.append(fail_str)\n",
    "\n",
    "print(\"right : \", right)\n",
    "print(\"mistake : \", mistake)\n",
    "print(\"accuracy:\", right/(right+mistake))\n",
    "\n",
    "df = pd.DataFrame({'index': index_list, \n",
    "                   'x_test_original_text': original_text_list, \n",
    "                   'probability': pred_prob_list, \n",
    "                   'pred_class': pred_class_list,\n",
    "                   'y_test': y_test_list,\n",
    "                   'is_fail': fail_str_list\n",
    "                  })\n",
    "\n",
    "df[['index', 'x_test_original_text','probability','pred_class','y_test','is_fail']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to get output of all the layers in the model for specific test input\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    def getLayerOutput(layer):\n",
    "        get_Layer_Output = k.function([model.layers[0].input], [layer.output])\n",
    "        return get_Layer_Output([x_test[0:1,]])[0]\n",
    "    \n",
    "    layer_output = []\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer_output.append(getLayerOutput(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the count of ouput. It should be equal to the number of layers\n",
    "len(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.5553787 , 0.18011284, 0.6420567 , ..., 0.55895984,\n",
       "          0.64191055, 0.5879252 ],\n",
       "         [0.5553787 , 0.18011284, 0.6420567 , ..., 0.55895984,\n",
       "          0.64191055, 0.5879252 ],\n",
       "         [0.5553787 , 0.18011284, 0.6420567 , ..., 0.55895984,\n",
       "          0.64191055, 0.5879252 ],\n",
       "         ...,\n",
       "         [0.38840163, 0.52240217, 0.18018687, ..., 0.7324097 ,\n",
       "          0.08540547, 0.76900613],\n",
       "         [0.43940055, 0.29121077, 0.47865772, ..., 0.3135028 ,\n",
       "          0.57172513, 0.8048034 ],\n",
       "         [0.6175225 , 0.5789665 , 0.5236012 , ..., 0.6587695 ,\n",
       "          0.22243512, 0.33174098]]], dtype=float32),\n",
       " array([[0.54188764, 0.25476873, 0.60314345, 0.5315438 , 0.81225485,\n",
       "         0.28845206, 0.14934397, 0.41312903, 0.15780509, 0.30540037,\n",
       "         0.11417412, 0.3650676 , 0.14723535, 0.5278066 , 0.5997043 ,\n",
       "         0.5833638 ]], dtype=float32),\n",
       " array([[0.12634443, 0.        , 0.        , 0.        , 0.684017  ,\n",
       "         0.26375854, 0.06051523, 0.07846066, 0.19827874, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.3097297 ,\n",
       "         0.19487377, 0.        , 0.        , 0.70889115, 0.        ,\n",
       "         0.09870214, 0.5269151 , 0.        , 0.        , 0.        ,\n",
       "         0.04202384, 0.        , 0.29322898, 0.09015664, 0.04474584,\n",
       "         0.        , 0.42819998, 0.07366087, 0.39347255, 0.        ,\n",
       "         0.        , 0.        , 0.39152402, 0.        , 0.        ,\n",
       "         0.41031033, 0.23007463, 0.        , 0.23913945, 0.        ,\n",
       "         0.6136151 , 0.        , 0.        , 0.        , 0.23703943,\n",
       "         0.        , 0.        , 0.0378878 , 0.18176587, 0.03147261,\n",
       "         0.        , 0.32030234, 0.        , 0.        , 0.1992329 ,\n",
       "         0.        , 0.1060954 , 0.        , 0.04277664]], dtype=float32),\n",
       " array([[0.12634443, 0.        , 0.        , 0.        , 0.684017  ,\n",
       "         0.26375854, 0.06051523, 0.07846066, 0.19827874, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.3097297 ,\n",
       "         0.19487377, 0.        , 0.        , 0.70889115, 0.        ,\n",
       "         0.09870214, 0.5269151 , 0.        , 0.        , 0.        ,\n",
       "         0.04202384, 0.        , 0.29322898, 0.09015664, 0.04474584,\n",
       "         0.        , 0.42819998, 0.07366087, 0.39347255, 0.        ,\n",
       "         0.        , 0.        , 0.39152402, 0.        , 0.        ,\n",
       "         0.41031033, 0.23007463, 0.        , 0.23913945, 0.        ,\n",
       "         0.6136151 , 0.        , 0.        , 0.        , 0.23703943,\n",
       "         0.        , 0.        , 0.0378878 , 0.18176587, 0.03147261,\n",
       "         0.        , 0.32030234, 0.        , 0.        , 0.1992329 ,\n",
       "         0.        , 0.1060954 , 0.        , 0.04277664]], dtype=float32),\n",
       " array([[0.        , 0.4768558 , 0.2663918 , 0.22146364, 0.14840367,\n",
       "         0.        , 0.        , 0.15654767, 0.23664755, 0.28393158,\n",
       "         0.        , 0.        , 0.52243376, 0.00539373, 0.12181561,\n",
       "         0.        , 0.08185294, 0.29976946, 0.5616941 , 0.33544374,\n",
       "         0.32786068, 0.        , 0.14473666, 0.        , 0.12925266,\n",
       "         0.43984178, 0.2042777 , 0.        , 0.        , 0.05423474,\n",
       "         0.03457484, 0.28308102]], dtype=float32),\n",
       " array([[0.23257595, 0.330117  , 0.02006472, 0.17145775, 0.        ,\n",
       "         0.3888613 , 0.        , 0.2162702 , 0.25190964, 0.        ,\n",
       "         0.25672403, 0.08934544, 0.25094375, 0.19024241, 0.6429679 ,\n",
       "         0.3307017 ]], dtype=float32),\n",
       " array([[0.5045035]], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check all the outputs\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12634443, 0.        , 0.        , 0.        , 0.684017  ,\n",
       "        0.26375854, 0.06051523, 0.07846066, 0.19827874, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3097297 ,\n",
       "        0.19487377, 0.        , 0.        , 0.70889115, 0.        ,\n",
       "        0.09870214, 0.5269151 , 0.        , 0.        , 0.        ,\n",
       "        0.04202384, 0.        , 0.29322898, 0.09015664, 0.04474584,\n",
       "        0.        , 0.42819998, 0.07366087, 0.39347255, 0.        ,\n",
       "        0.        , 0.        , 0.39152402, 0.        , 0.        ,\n",
       "        0.41031033, 0.23007463, 0.        , 0.23913945, 0.        ,\n",
       "        0.6136151 , 0.        , 0.        , 0.        , 0.23703943,\n",
       "        0.        , 0.        , 0.0378878 , 0.18176587, 0.03147261,\n",
       "        0.        , 0.32030234, 0.        , 0.        , 0.1992329 ,\n",
       "        0.        , 0.1060954 , 0.        , 0.04277664]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the specific layer output\n",
    "layer_output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_Sentiment_Classification_using_Sequential_Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
